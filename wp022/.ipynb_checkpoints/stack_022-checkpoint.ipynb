{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simpler modelling stack, where models have been pulled from the stack if they are not useful.  Strangely, the regularization on the top level of the stack must be light l1.  Produces 0.28966 in local test. Just over 0.285 on LB.  Position: 387 (386 was bottom of bronze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd,matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(array):\n",
    "    srtInd = array.argsort()\n",
    "    ranks = np.empty(len(array), float)\n",
    "    ranks[srtInd] = np.arange(len(array))\n",
    "    return ranks / float(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, n_splits, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.pipelines= []\n",
    "        self.base_models= []\n",
    "        for x in base_models:\n",
    "            self.pipelines.append( x[0] )\n",
    "            self.base_models.append( x[1] )\n",
    "\n",
    "    def fit_predict(self, df_train, y, df_test):\n",
    "        y = np.array(y)\n",
    "\n",
    "        folds = StratifiedKFold(y,n_folds=self.n_splits, shuffle=True, random_state=2016)\n",
    "\n",
    "        S_train = np.zeros((len(df_train.index), len(self.base_models)))\n",
    "        S_test = np.zeros((len(df_test.index), len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            #\n",
    "            PIPELINE= self.pipelines[i]\n",
    "            #\n",
    "            S_test_i = np.zeros((len(df_test.index), self.n_splits))\n",
    "#             X_train, y_train, X_test, y_test= PIPELINE(df_train.copy(),df_test.copy())\n",
    "            #\n",
    "            for j, (train_idx, val_idx) in enumerate(folds):\n",
    "                #\n",
    "                X_train, y_train, X_holdout, y_holdout= PIPELINE(\n",
    "                    df_train.copy().loc[train_idx,:],\n",
    "                    df_train.copy().loc[val_idx,:]\n",
    "                )\n",
    "                _, _, X_test, y_test= PIPELINE(df_train.copy().loc[train_idx,:],df_test.copy()\n",
    "                )\n",
    "                #\n",
    "                print (\"Fit %s --> %s fold %d\" % (str(PIPELINE).split()[1],str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]              \n",
    "\n",
    "                S_train[val_idx, i] = y_pred\n",
    "                #\n",
    "                test_probs= clf.predict_proba(X_test)[:,1]\n",
    "                S_test_i[:, j] = np.log(test_probs) - np.log(1.0 - test_probs)\n",
    "            agg_lor= S_test_i.mean(axis=1)\n",
    "            S_test[:, i] = 1.0 / (1.0 + np.exp( -agg_lor) )\n",
    "        return pd.DataFrame(S_train,columns=['x_'+str(i) for i in range(np.shape(S_train)[1])]),pd.DataFrame(S_test,columns=['x_'+str(i) for i in range(np.shape(S_test)[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add output of layer results ofr quick iteration\n",
    "class Stack:\n",
    "    def __init__(self,k_folds,hidden_layers,top_layer,saveInternalVectors=False):\n",
    "        self.saveInternalVectors= saveInternalVectors\n",
    "        self.layers= []\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append( Layer(k_folds,h) )\n",
    "        self.top_layer= top_layer \n",
    "        return None\n",
    "    \n",
    "    def fit_predict(self,df_train,y,df_test,external_base_scores= None):\n",
    "        Xt_train= df_train\n",
    "        Xt_test= df_test\n",
    "        i= 1\n",
    "        for layer in self.layers:\n",
    "            print 'Fitting stack layer '+str(i)\n",
    "            Xt_train, Xt_test= layer.fit_predict(Xt_train,y,Xt_test)\n",
    "            Xt_train.loc[:,'target']= y\n",
    "            if external_base_scores is not None and i==1:\n",
    "                Xt_train.loc[:,'ext']= external_base_scores[0]\n",
    "                Xt_test.loc[:,'ext']= external_base_scores[1]\n",
    "#                 Xt_train= np.concatenate( (Xt_train,np.reshape(external_base_scores[0],\n",
    "#                                                                (np.shape(external_base_scores[0])[0],1))),\n",
    "#                                          axis=1)\n",
    "#                 Xt_test= np.concatenate( (Xt_test,np.reshape(external_base_scores[1],\n",
    "#                                                             (np.shape(external_base_scores[1])[0],1))), \n",
    "#                                         axis=1)\n",
    "            #\n",
    "            if self.saveInternalVectors:\n",
    "                fname= 'STACK_internal_train_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_train)\n",
    "                fname= 'STACK_internal_test_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_test)\n",
    "            i+=1\n",
    "        Xt_train.drop('target',axis=1,inplace=True)\n",
    "        self.top_layer.fit(Xt_train.as_matrix(),y)\n",
    "        return self.top_layer.predict_proba(Xt_test.as_matrix())[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now specify the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now score the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Read data\n",
    "\n",
    "# # train\n",
    "# X_train= np.load(open('../WP014/dummy_train_matrix.bin','rb'))\n",
    "# y_train= np.load(open('../WP014/dummy_train_labels.bin','rb'))\n",
    "\n",
    "# # test\n",
    "# X_test= np.load(open('../WP014/dummy_test_matrix.bin','rb'))\n",
    "# y_test= np.load(open('../WP014/dummy_test_labels.bin','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection, pipe0_median as pipe0\n",
    "\n",
    "df_train= pd.read_csv('../data/train.csv')\n",
    "df_test= pd.read_csv('../data/test.csv')\n",
    "\n",
    "# index_train, index_test= sklearn.model_selection.train_test_split( range(len(df_train.index)) , \n",
    "#                                                                     test_size=0.3,random_state=1)\n",
    "\n",
    "# df_test= df_train.loc[index_test,:].reset_index(drop=True)\n",
    "# y_test= df_test.target.values\n",
    "# df_train= df_train.loc[index_train,:].reset_index(drop=True)\n",
    "# y_train= df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read rgf pipeline train and test scores\n",
    "X_rgf_test= pd.read_csv('../wp018/output_0.01_300leaf/rgf_blind_scores.csv').target.values\n",
    "X_rgf_train= pd.read_csv('../wp018/output_0.01_300leaf/rgf_validation_ scores.csv').target.values\n",
    "# X_rgf_test= X_rgf_train.target.values[index_test]\n",
    "# X_rgf_train= X_rgf_train.target.values[index_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ids= pd.read_csv('../data/test.csv',usecols=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm.sklearn\n",
    "import xgboost.sklearn\n",
    "import catboost\n",
    "import sklearn.linear_model, sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.01\n",
    "lgb_params['n_estimators'] = 1300\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['num_leaves']= 25\n",
    "lgb_params['n_jobs']=8\n",
    "\n",
    "\n",
    "# lgb_params_3 = {\n",
    "#     'learning_rate': 0.02,\n",
    "#     'n_estimators': 800,\n",
    "#     'max_depth': 4,\n",
    "#     'n_jobs':8\n",
    "# }\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 800, #150,\n",
    "    'max_depth': 4,\n",
    "    'n_jobs':8\n",
    "#     'min_child_samples':100\n",
    "}\n",
    "\n",
    "lgb_params_4 = {\n",
    "    'learning_rate':0.05,\n",
    "    'n_estimators':600,\n",
    "    'num_leaves':35,\n",
    "    'min_child_samples':500,\n",
    "    'n_jobs':8\n",
    "}\n",
    "\n",
    "\n",
    "xgb_params= {'learning_rate': 0.07,\n",
    "             'n_estimators':525,\n",
    "             'max_depth': 4, \n",
    "             'nthread':8,\n",
    "             'subsample': 0.8,\n",
    "             'min_child_weight':6.0, \n",
    "             'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', \n",
    "             'eval_metric': 'auc', \n",
    "             'seed': 99, \n",
    "             'silent': True,\n",
    "             'scale_pos_weight': 1.6,\n",
    "             'reg_alpha':8,\n",
    "             'reg_lambda':1.3,\n",
    "             'gamma':10\n",
    "            }\n",
    "\n",
    "cb_params= {\n",
    "    'learning_rate':0.05, \n",
    "    'depth':6, \n",
    "    'l2_leaf_reg': 14, \n",
    "    'iterations': 650,\n",
    "    'verbose': False,\n",
    "    'loss_function':'Logloss'\n",
    "    }\n",
    "\n",
    "# lgbmn_params= {'num_leaves': 81, 'verbose': 1, 'learning_rate': 0.005, \n",
    "#                'min_data': 650, 'categorical_column': [], 'bagging_fraction': 0.9, \n",
    "#                'metric': ['auc'], 'boosting_type': 'gbdt', 'lambda_l1': 30,\n",
    "#                'bagging_freq': 3, 'lambda_l2': 0, 'is_unbalance': True, \n",
    "#                'max_bin': 255, 'objective': ['binary'], 'max_depth': 6, \n",
    "#                'feature_fraction': 0.7,'n_estimators':1600\n",
    "#               }\n",
    "\n",
    "# Layer 1\n",
    "lgbm1 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params))\n",
    "xgbm1= (pipe0.run_pipe0,   xgboost.sklearn.XGBClassifier(**xgb_params))\n",
    "lgbm3 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params_3))\n",
    "lgbm4 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params_4))\n",
    "cb= (pipe0.run_pipe0,      catboost.CatBoostClassifier(**cb_params))\n",
    "# lgbmn = (pipe3.run_pipe3,  lightgbm.sklearn.LGBMClassifier(**lgbmn_params))\n",
    "\n",
    "# Top layer\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.0,class_weight='balanced',penalty='l1')\n",
    "\n",
    "# Define the stack\n",
    "stack = Stack(10,[ [cb,xgbm1,lgbm1,lgbm3,lgbm4] ], stacker, saveInternalVectors=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stack layer 1\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 1\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 2\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 3\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 4\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 5\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 6\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 7\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 8\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 9\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1134c9c90> fold 10\n",
      "Fit run_pipe0 --> XGBClassifier fold 1\n",
      "Fit run_pipe0 --> XGBClassifier fold 2\n",
      "Fit run_pipe0 --> XGBClassifier fold 3\n",
      "Fit run_pipe0 --> XGBClassifier fold 4\n",
      "Fit run_pipe0 --> XGBClassifier fold 5\n",
      "Fit run_pipe0 --> XGBClassifier fold 6\n",
      "Fit run_pipe0 --> XGBClassifier fold 7\n",
      "Fit run_pipe0 --> XGBClassifier fold 8\n",
      "Fit run_pipe0 --> XGBClassifier fold 9\n",
      "Fit run_pipe0 --> XGBClassifier fold 10\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 4\n",
      "Fit run_pipe0 --> LGBMClassifier fold 5\n",
      "Fit run_pipe0 --> LGBMClassifier fold 6\n",
      "Fit run_pipe0 --> LGBMClassifier fold 7\n",
      "Fit run_pipe0 --> LGBMClassifier fold 8\n",
      "Fit run_pipe0 --> LGBMClassifier fold 9\n",
      "Fit run_pipe0 --> LGBMClassifier fold 10\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 4\n",
      "Fit run_pipe0 --> LGBMClassifier fold 5\n",
      "Fit run_pipe0 --> LGBMClassifier fold 6\n",
      "Fit run_pipe0 --> LGBMClassifier fold 7\n",
      "Fit run_pipe0 --> LGBMClassifier fold 8\n",
      "Fit run_pipe0 --> LGBMClassifier fold 9\n",
      "Fit run_pipe0 --> LGBMClassifier fold 10\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 4\n",
      "Fit run_pipe0 --> LGBMClassifier fold 5\n",
      "Fit run_pipe0 --> LGBMClassifier fold 6\n",
      "Fit run_pipe0 --> LGBMClassifier fold 7\n",
      "Fit run_pipe0 --> LGBMClassifier fold 8\n",
      "Fit run_pipe0 --> LGBMClassifier fold 9\n",
      "Fit run_pipe0 --> LGBMClassifier fold 10\n",
      " Training took 7458.395422\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "from datetime import datetime\n",
    "tc= datetime.now()\n",
    "y_pred = stack.fit_predict(df_train, df_train.target.values, df_test,\n",
    "                            external_base_scores= (X_rgf_train, X_rgf_test) ) #,\n",
    "print' Training took '+str( (datetime.now() - tc).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Gini score= "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3292168fb584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Test Gini score= '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.290426135503\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1.,0.290426135503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l2')\n",
    "X_train= np.load('STACK_internal_train_layer_1.bin')\n",
    "X_test= np.load('STACK_internal_test_layer_1.bin')\n",
    "X_test[:,5]= X_rgf_test\n",
    "X_train[:,6]= X_rgf_train\n",
    "y_train= X_train[:,5]\n",
    "X_train= np.delete(X_train,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sklearn.preprocessing\n",
    "# fvg= sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "# X_train_pol= fvg.fit_transform(X_train)\n",
    "# X_test_pol= fvg.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Gini score=  0.286672942747 0.286699739102 0.282201168818\n"
     ]
    }
   ],
   "source": [
    "#  This for testing ensemble incl RGF\n",
    "internal= [0,1,2,3,4,5]\n",
    "folds = StratifiedKFold(y_train,n_folds=10, shuffle=True, random_state=2016)\n",
    "y_pred= np.zeros(len(y_train))\n",
    "for i, (itr,ite) in enumerate(folds):\n",
    "    stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l2')\n",
    "    stacker.fit( X_train[np.ix_(itr,internal)] , y_train[itr] )\n",
    "    y_pred[ite]= stacker.predict_proba(X_train[np.ix_(ite,internal)])[:,1]\n",
    "print 'CV Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train,y_pred)-1.,0.286699739102,0.282201168818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try the bagging approach (which will take longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took:  58.043924\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "from datetime import datetime,timedelta\n",
    "tc= datetime.now()\n",
    "clf= sklearn.ensemble.BaggingClassifier(stacker,\n",
    "                                        n_estimators=128,\n",
    "                                        oob_score=True,\n",
    "                                        max_features=4,\n",
    "                                        max_samples=0.6,\n",
    "                                        random_state=1,\n",
    "                                        n_jobs=-1\n",
    "                                       )\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred= clf.predict_proba(X_test)[:,1]\n",
    "print 'Training took: ',(datetime.now() - tc).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595212 595212\n"
     ]
    }
   ],
   "source": [
    "y_pred2= clf.oob_decision_function_[:,1]\n",
    "ok= np.where(map(np.isfinite,y_pred2))[0]\n",
    "print len(ok),len(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Gini score=  0.290101648572 0.286463220363 (0.286760096353)\n"
     ]
    }
   ],
   "source": [
    "print 'OOB Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train[ok],y_pred2[ok])-1.,0.286463220363,'(0.286760096353)'\n",
    "# print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1.,0.291740847787,'(0.292343541304)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took:  264.259035\n"
     ]
    }
   ],
   "source": [
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 115,\n",
    "    'max_depth': 1,\n",
    "    'n_jobs':1\n",
    "#     'eval_metric':'auc'\n",
    "}\n",
    "altStacker= xgboost.sklearn.XGBClassifier(**lgb_params_3)\n",
    "tc= datetime.now()\n",
    "clf2= sklearn.ensemble.BaggingClassifier(altStacker,\n",
    "                                        n_estimators=128,\n",
    "                                        oob_score=True,\n",
    "                                        max_features=4,\n",
    "                                        max_samples=0.6,\n",
    "                                        random_state=1,\n",
    "                                        n_jobs=-1\n",
    "                                       )\n",
    "clf2.fit(X_train,y_train)\n",
    "y_pred2= clf2.predict_proba(X_test)[:,1]\n",
    "print 'Training took: ',(datetime.now() - tc).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took:  158.846881\n",
      "Test Gini score=  0.293042158459 0.293227932709\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neural_network\n",
    "alt2Stacker= sklearn.neural_network.MLPClassifier(hidden_layer_sizes=[7,50],alpha=1.E-4)\n",
    "tc= datetime.now()\n",
    "clf3= sklearn.ensemble.BaggingClassifier(alt2Stacker,\n",
    "                                        n_estimators=128,\n",
    "                                        oob_score=True,\n",
    "                                        max_features=4,\n",
    "                                        max_samples=0.6,\n",
    "                                        random_state=1,\n",
    "                                        n_jobs=-1\n",
    "                                       )\n",
    "clf3.fit(X_train,y_train)\n",
    "y_pred3= clf3.predict_proba(X_test)[:,1]\n",
    "print 'Training took: ',(datetime.now() - tc).total_seconds()\n",
    "print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred3)-1.,0.293227932709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Gini score=  0.286710475971 0.286463220363 (0.286760096353)\n"
     ]
    }
   ],
   "source": [
    "print 'OOB Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train,oob1)-1.,0.286463220363,'(0.286760096353)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Gini score=  0.292561173345 0.293267930978\n"
     ]
    }
   ],
   "source": [
    "oob1= clf.oob_decision_function_[:,1]\n",
    "oob2= clf2.oob_decision_function_[:,1]\n",
    "oob3= clf3.oob_decision_function_[:,1]\n",
    "top= sklearn.linear_model.LogisticRegression(C=100000000,class_weight='balanced',penalty='l1')\n",
    "top.fit(pd.DataFrame({'1':oob1,'2':oob2,'3':oob3}).as_matrix(),y_train)\n",
    "top_pred= top.predict_proba(pd.DataFrame({'1':y_pred,'2':y_pred2,'3':y_pred3}).as_matrix())[:,1]\n",
    "print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,top_pred)-1.,0.293267930978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Gini score=  0.293257947577\n"
     ]
    }
   ],
   "source": [
    "print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred2)-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...and now output predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# out_df= pd.DataFrame( {'id': test_ids.id.values, 'target': y_pred},\n",
    "#                      columns=['id','target']\n",
    "#                     ).to_csv('submission_wp021c.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
