{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simpler modelling stack, where models have been pulled from the stack if they are not useful.  Strangely, the regularization on the top level of the stack must be light l1.  Produces 0.28966 in local test. Just over 0.285 on LB.  Position: 387 (386 was bottom of bronze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd,matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(array):\n",
    "    srtInd = array.argsort()\n",
    "    ranks = np.empty(len(array), float)\n",
    "    ranks[srtInd] = np.arange(len(array))\n",
    "    return ranks / float(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, n_splits, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.pipelines= []\n",
    "        self.base_models= []\n",
    "        for x in base_models:\n",
    "            self.pipelines.append( x[0] )\n",
    "            self.base_models.append( x[1] )\n",
    "\n",
    "    def fit_predict(self, df_train, y, df_test):\n",
    "        y = np.array(y)\n",
    "\n",
    "        folds = StratifiedKFold(y,n_folds=self.n_splits, shuffle=True, random_state=2016)\n",
    "\n",
    "        S_train = np.zeros((len(df_train.index), len(self.base_models)))\n",
    "        S_test = np.zeros((len(df_test.index), len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            #\n",
    "            PIPELINE= self.pipelines[i]\n",
    "            #\n",
    "            S_test_i = np.zeros((len(df_test.index), self.n_splits))\n",
    "#             X_train, y_train, X_test, y_test= PIPELINE(df_train.copy(),df_test.copy())\n",
    "            #\n",
    "            for j, (train_idx, val_idx) in enumerate(folds):\n",
    "                #\n",
    "                X_train, y_train, X_holdout, y_holdout= PIPELINE(\n",
    "                    df_train.copy().loc[train_idx,:],\n",
    "                    df_train.copy().loc[val_idx,:]\n",
    "                )\n",
    "                _, _, X_test, y_test= PIPELINE(df_train.copy().loc[train_idx,:],df_test.copy()\n",
    "                )\n",
    "                #\n",
    "                print (\"Fit %s --> %s fold %d\" % (str(PIPELINE).split()[1],str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]              \n",
    "\n",
    "                S_train[val_idx, i] = y_pred\n",
    "                #\n",
    "                test_probs= clf.predict_proba(X_test)[:,1]\n",
    "                S_test_i[:, j] = np.log(test_probs) - np.log(1.0 - test_probs)\n",
    "            agg_lor= S_test_i.mean(axis=1)\n",
    "            S_test[:, i] = 1.0 / (1.0 + np.exp( -agg_lor) )\n",
    "        return pd.DataFrame(S_train,columns=['x_'+str(i) for i in range(np.shape(S_train)[1])]),pd.DataFrame(S_test,columns=['x_'+str(i) for i in range(np.shape(S_test)[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add output of layer results ofr quick iteration\n",
    "class Stack:\n",
    "    def __init__(self,k_folds,hidden_layers,top_layer,saveInternalVectors=False):\n",
    "        self.saveInternalVectors= saveInternalVectors\n",
    "        self.layers= []\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append( Layer(k_folds,h) )\n",
    "        self.top_layer= top_layer \n",
    "        return None\n",
    "    \n",
    "    def fit_predict(self,df_train,y,df_test,external_base_scores= None):\n",
    "        Xt_train= df_train\n",
    "        Xt_test= df_test\n",
    "        i= 1\n",
    "        for layer in self.layers:\n",
    "            print 'Fitting stack layer '+str(i)\n",
    "            Xt_train, Xt_test= layer.fit_predict(Xt_train,y,Xt_test)\n",
    "            Xt_train.loc[:,'target']= y\n",
    "            if external_base_scores is not None and i==1:\n",
    "                Xt_train.loc[:,'ext']= external_base_scores[0]\n",
    "                Xt_test.loc[:,'ext']= external_base_scores[1]\n",
    "#                 Xt_train= np.concatenate( (Xt_train,np.reshape(external_base_scores[0],\n",
    "#                                                                (np.shape(external_base_scores[0])[0],1))),\n",
    "#                                          axis=1)\n",
    "#                 Xt_test= np.concatenate( (Xt_test,np.reshape(external_base_scores[1],\n",
    "#                                                             (np.shape(external_base_scores[1])[0],1))), \n",
    "#                                         axis=1)\n",
    "            #\n",
    "            if self.saveInternalVectors:\n",
    "                fname= 'STACK_internal_train_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_train)\n",
    "                fname= 'STACK_internal_test_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_test)\n",
    "            i+=1\n",
    "        Xt_train.drop('target',axis=1,inplace=True)\n",
    "        self.top_layer.fit(Xt_train.as_matrix(),y)\n",
    "        return self.top_layer.predict_proba(Xt_test.as_matrix())[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now specify the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now score the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Read data\n",
    "\n",
    "# # train\n",
    "# X_train= np.load(open('../WP014/dummy_train_matrix.bin','rb'))\n",
    "# y_train= np.load(open('../WP014/dummy_train_labels.bin','rb'))\n",
    "\n",
    "# # test\n",
    "# X_test= np.load(open('../WP014/dummy_test_matrix.bin','rb'))\n",
    "# y_test= np.load(open('../WP014/dummy_test_labels.bin','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection, pipe0_median as pipe0\n",
    "\n",
    "df_train= pd.read_csv('../data/train.csv')\n",
    "df_test= pd.read_csv('../data/test.csv')\n",
    "\n",
    "# index_train, index_test= sklearn.model_selection.train_test_split( range(len(df_train.index)) , \n",
    "#                                                                     test_size=0.3,random_state=1)\n",
    "\n",
    "# df_test= df_train.loc[index_test,:].reset_index(drop=True)\n",
    "# y_test= df_test.target.values\n",
    "# df_train= df_train.loc[index_train,:].reset_index(drop=True)\n",
    "# y_train= df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read rgf pipeline train and test scores\n",
    "# X_rgf_train= pd.read_csv('../wp017/rgf_scores_train.csv').target.values\n",
    "# X_rgf_test= pd.read_csv('../wp017/rgf_blind_scores.csv').target.values\n",
    "X_rgf_test= pd.read_csv('../wp018/output_0.01_300leaf/rgf_blind_scores.csv').target.values\n",
    "X_rgf_train= pd.read_csv('../wp018/output_0.01_300leaf/rgf_validation_ scores.csv').target.values\n",
    "# X_rgf_test= X_rgf_train.target.values[index_test]\n",
    "# X_rgf_train= X_rgf_train.target.values[index_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ids= pd.read_csv('../data/test.csv',usecols=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm.sklearn\n",
    "import xgboost.sklearn\n",
    "import catboost\n",
    "import sklearn.linear_model, sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.01\n",
    "lgb_params['n_estimators'] = 1300\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['num_leaves']= 25\n",
    "lgb_params['n_jobs']=8\n",
    "\n",
    "\n",
    "# lgb_params_3 = {\n",
    "#     'learning_rate': 0.02,\n",
    "#     'n_estimators': 800,\n",
    "#     'max_depth': 4,\n",
    "#     'n_jobs':8\n",
    "# }\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 800, #150,\n",
    "    'max_depth': 4,\n",
    "    'n_jobs':8\n",
    "#     'min_child_samples':100\n",
    "}\n",
    "\n",
    "lgb_params_4 = {\n",
    "    'learning_rate':0.05,\n",
    "    'n_estimators':600,\n",
    "    'num_leaves':35,\n",
    "    'min_child_samples':500,\n",
    "    'n_jobs':8\n",
    "}\n",
    "\n",
    "\n",
    "xgb_params= {'learning_rate': 0.07,\n",
    "             'n_estimators':525,\n",
    "             'max_depth': 4, \n",
    "             'nthread':8,\n",
    "             'subsample': 0.8,\n",
    "             'min_child_weight':6.0, \n",
    "             'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', \n",
    "             'eval_metric': 'auc', \n",
    "             'seed': 99, \n",
    "             'silent': True,\n",
    "             'scale_pos_weight': 1.6,\n",
    "             'reg_alpha':8,\n",
    "             'reg_lambda':1.3,\n",
    "             'gamma':10\n",
    "            }\n",
    "\n",
    "cb_params= {\n",
    "    'learning_rate':0.05, \n",
    "    'depth':6, \n",
    "    'l2_leaf_reg': 14, \n",
    "    'iterations': 650,\n",
    "    'verbose': False,\n",
    "    'loss_function':'Logloss'\n",
    "    }\n",
    "\n",
    "# lgbmn_params= {'num_leaves': 81, 'verbose': 1, 'learning_rate': 0.005, \n",
    "#                'min_data': 650, 'categorical_column': [], 'bagging_fraction': 0.9, \n",
    "#                'metric': ['auc'], 'boosting_type': 'gbdt', 'lambda_l1': 30,\n",
    "#                'bagging_freq': 3, 'lambda_l2': 0, 'is_unbalance': True, \n",
    "#                'max_bin': 255, 'objective': ['binary'], 'max_depth': 6, \n",
    "#                'feature_fraction': 0.7,'n_estimators':1600\n",
    "#               }\n",
    "\n",
    "# Layer 1\n",
    "lgbm1 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params))\n",
    "xgbm1= (pipe0.run_pipe0,   xgboost.sklearn.XGBClassifier(**xgb_params))\n",
    "lgbm3 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params_3))\n",
    "lgbm4 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params_4))\n",
    "cb= (pipe0.run_pipe0,      catboost.CatBoostClassifier(**cb_params))\n",
    "# lgbmn = (pipe3.run_pipe3,  lightgbm.sklearn.LGBMClassifier(**lgbmn_params))\n",
    "\n",
    "# Top layer\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.0,class_weight='balanced',penalty='l1')\n",
    "\n",
    "# Define the stack\n",
    "stack = Stack(10,[ [cb,xgbm1,lgbm1,lgbm3,lgbm4] ], stacker, saveInternalVectors=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stack layer 1\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 1\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 2\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 3\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 4\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 5\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 6\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 7\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 8\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 9\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x1130a7d50> fold 10\n",
      "Fit run_pipe0 --> XGBClassifier fold 1\n",
      "Fit run_pipe0 --> XGBClassifier fold 2\n",
      "Fit run_pipe0 --> XGBClassifier fold 3\n",
      "Fit run_pipe0 --> XGBClassifier fold 4\n",
      "Fit run_pipe0 --> XGBClassifier fold 5\n",
      "Fit run_pipe0 --> XGBClassifier fold 6\n",
      "Fit run_pipe0 --> XGBClassifier fold 7\n",
      "Fit run_pipe0 --> XGBClassifier fold 8\n",
      "Fit run_pipe0 --> XGBClassifier fold 9\n",
      "Fit run_pipe0 --> XGBClassifier fold 10\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 4\n",
      "Fit run_pipe0 --> LGBMClassifier fold 5\n",
      "Fit run_pipe0 --> LGBMClassifier fold 6\n",
      "Fit run_pipe0 --> LGBMClassifier fold 7\n",
      "Fit run_pipe0 --> LGBMClassifier fold 8\n",
      "Fit run_pipe0 --> LGBMClassifier fold 9\n",
      "Fit run_pipe0 --> LGBMClassifier fold 10\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 4\n",
      "Fit run_pipe0 --> LGBMClassifier fold 5\n",
      "Fit run_pipe0 --> LGBMClassifier fold 6\n",
      "Fit run_pipe0 --> LGBMClassifier fold 7\n",
      "Fit run_pipe0 --> LGBMClassifier fold 8\n",
      "Fit run_pipe0 --> LGBMClassifier fold 9\n",
      "Fit run_pipe0 --> LGBMClassifier fold 10\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 4\n",
      "Fit run_pipe0 --> LGBMClassifier fold 5\n",
      "Fit run_pipe0 --> LGBMClassifier fold 6\n",
      "Fit run_pipe0 --> LGBMClassifier fold 7\n",
      "Fit run_pipe0 --> LGBMClassifier fold 8\n",
      "Fit run_pipe0 --> LGBMClassifier fold 9\n",
      "Fit run_pipe0 --> LGBMClassifier fold 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-63d533b810a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m y_pred = stack.fit_predict(df_train, df_train.target.values, df_test,\n\u001b[0;32m----> 5\u001b[0;31m                             external_base_scores= (X_rgf_train, X_rgf_test) ) #,\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m' Training took '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6ce674904bd5>\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, df_train, y, df_test, external_base_scores)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mXt_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    541\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 43\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "from datetime import datetime\n",
    "tc= datetime.now()\n",
    "y_pred = stack.fit_predict(df_train, df_train.target.values, df_test,\n",
    "                            external_base_scores= (X_rgf_train, X_rgf_test) ) #,\n",
    "print' Training took '+str( (datetime.now() - tc).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1.,0.290426135503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l2')\n",
    "X_train= np.load('internals_withrgf_median_test_run4/STACK_internal_train_layer_1.bin')\n",
    "X_test= np.load('internals_withrgf_median_test_run4/STACK_internal_test_layer_1.bin')\n",
    "X_test[:,5]= X_rgf_test\n",
    "X_train[:,6]= X_rgf_train\n",
    "y_train= X_train[:,5]\n",
    "X_train= np.delete(X_train,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OOF\n",
    "X_harl= pd.read_csv('../wp023/internals_test/harless_oob.csv').target.values\n",
    "X_train= np.column_stack((X_train,X_harl))\n",
    "X_froza= pd.read_csv('../wp023/internals_test/froza_oof.csv').target.values\n",
    "X_train= np.column_stack((X_train,X_froza))\n",
    "#\n",
    "# Test\n",
    "X_harl= pd.read_csv('../wp023/internals_test/harless_test.csv').target.values\n",
    "X_test= np.column_stack((X_test,X_harl))\n",
    "X_froza= pd.read_csv('../wp023/internals_test/froza_test.csv').target.values\n",
    "X_test= np.column_stack((X_test,X_froza))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sklearn.preprocessing\n",
    "# fvg= sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "# X_train_pol= fvg.fit_transform(X_train)\n",
    "# X_test_pol= fvg.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Gini score=  0.290674519781 0.286699739102 0.282201168818 0.289731866932\n"
     ]
    }
   ],
   "source": [
    "#  This for testing ensemble incl RGF\n",
    "internal= [0,1,2,3,4,5,6,7]\n",
    "folds = StratifiedKFold(y_train,n_folds=10, shuffle=True, random_state=2016)\n",
    "y_pred= np.zeros(len(y_train))\n",
    "for i, (itr,ite) in enumerate(folds):\n",
    "    stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l2')\n",
    "    stacker.fit( X_train[np.ix_(itr,internal)] , y_train[itr] )\n",
    "    y_pred[ite]= stacker.predict_proba(X_train[np.ix_(ite,internal)])[:,1]\n",
    "print 'CV Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train,y_pred)-1.,0.286699739102,0.282201168818,0.289731866932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try the bagging approach (which will take longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took:  103.627528\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "from datetime import datetime,timedelta\n",
    "tc= datetime.now()\n",
    "# lgb_params_3 = {\n",
    "#     'learning_rate': 0.1,\n",
    "#     'n_estimators': 115,\n",
    "#     'max_depth': 1,\n",
    "#     'n_jobs':1\n",
    "# #     'eval_metric':'auc'\n",
    "# }\n",
    "# altStacker= xgboost.sklearn.XGBClassifier(**lgb_params_3)\n",
    "clf= sklearn.ensemble.BaggingClassifier(stacker,\n",
    "                                        n_estimators=128,\n",
    "                                        oob_score=True,\n",
    "                                        max_features=4,\n",
    "                                        max_samples=1.0,\n",
    "                                        random_state=3,\n",
    "                                        n_jobs=-1\n",
    "                                       )\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred= clf.predict_proba(X_test)[:,1]\n",
    "print 'Training took: ',(datetime.now() - tc).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595212 595212\n"
     ]
    }
   ],
   "source": [
    "y_pred2= clf.oob_decision_function_[:,1]\n",
    "ok= np.where(map(np.isfinite,y_pred2))[0]\n",
    "print len(ok),len(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Gini score=  0.290849329423 0.286463220363 (0.286760096353) 0.291032405649\n"
     ]
    }
   ],
   "source": [
    "print 'OOB Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train[ok],y_pred2[ok])-1.,0.286463220363,'(0.286760096353)',0.291032405649\n",
    "# print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1.,0.291740847787,'(0.292343541304)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...and now output predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# out_df= pd.DataFrame( {'id': test_ids.id.values, 'target': y_pred},\n",
    "#                      columns=['id','target']\n",
    "#                     ).to_csv('submission_wp024c.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
