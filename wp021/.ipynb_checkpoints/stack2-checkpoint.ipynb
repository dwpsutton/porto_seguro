{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simpler modelling stack, where models have been pulled from the stack if they are not useful.  Strangely, the regularization on the top level of the stack must be light l1.  Produces 0.28966 in local test. Just over 0.285 on LB.  Position: 387 (386 was bottom of bronze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd,matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(array):\n",
    "    srtInd = array.argsort()\n",
    "    ranks = np.empty(len(array), float)\n",
    "    ranks[srtInd] = np.arange(len(array))\n",
    "    return ranks / float(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, n_splits, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.pipelines= []\n",
    "        self.base_models= []\n",
    "        for x in base_models:\n",
    "            self.pipelines.append( x[0] )\n",
    "            self.base_models.append( x[1] )\n",
    "\n",
    "    def fit_predict(self, df_train, y, df_test):\n",
    "        y = np.array(y)\n",
    "\n",
    "        folds = StratifiedKFold(y,n_folds=self.n_splits, shuffle=True, random_state=2016)\n",
    "\n",
    "        S_train = np.zeros((len(df_train.index), len(self.base_models)))\n",
    "        S_test = np.zeros((len(df_test.index), len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            #\n",
    "            PIPELINE= self.pipelines[i]\n",
    "            #\n",
    "            S_test_i = np.zeros((len(df_test.index), self.n_splits))\n",
    "#             X_train, y_train, X_test, y_test= PIPELINE(df_train.copy(),df_test.copy())\n",
    "            #\n",
    "            for j, (train_idx, val_idx) in enumerate(folds):\n",
    "                #\n",
    "                X_train, y_train, X_holdout, y_holdout= PIPELINE(\n",
    "                    df_train.copy().loc[train_idx,:],\n",
    "                    df_train.copy().loc[val_idx,:]\n",
    "                )\n",
    "                _, _, X_test, y_test= PIPELINE(df_train.copy().loc[train_idx,:],df_test.copy()\n",
    "                )\n",
    "                #\n",
    "                print (\"Fit %s --> %s fold %d\" % (str(PIPELINE).split()[1],str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]              \n",
    "\n",
    "                S_train[val_idx, i] = y_pred\n",
    "                #\n",
    "                test_probs= clf.predict_proba(X_test)[:,1]\n",
    "                S_test_i[:, j] = np.log(test_probs) - np.log(1.0 - test_probs)\n",
    "            agg_lor= S_test_i.mean(axis=1)\n",
    "            S_test[:, i] = 1.0 / (1.0 + np.exp( -agg_lor) )\n",
    "        return pd.DataFrame(S_train,columns=['x_'+str(i) for i in range(np.shape(S_train)[1])]),pd.DataFrame(S_test,columns=['x_'+str(i) for i in range(np.shape(S_test)[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add output of layer results ofr quick iteration\n",
    "class Stack:\n",
    "    def __init__(self,k_folds,hidden_layers,top_layer,saveInternalVectors=False):\n",
    "        self.saveInternalVectors= saveInternalVectors\n",
    "        self.layers= []\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append( Layer(k_folds,h) )\n",
    "        self.top_layer= top_layer \n",
    "        return None\n",
    "    \n",
    "    def fit_predict(self,df_train,y,df_test,external_base_scores= None):\n",
    "        Xt_train= df_train\n",
    "        Xt_test= df_test\n",
    "        i= 1\n",
    "        for layer in self.layers:\n",
    "            print 'Fitting stack layer '+str(i)\n",
    "            Xt_train, Xt_test= layer.fit_predict(Xt_train,y,Xt_test)\n",
    "            Xt_train.loc[:,'target']= y\n",
    "            if external_base_scores is not None and i==1:\n",
    "                Xt_train.loc[:,'ext']= external_base_scores[0]\n",
    "                Xt_test.loc[:,'ext']= external_base_scores[1]\n",
    "#                 Xt_train= np.concatenate( (Xt_train,np.reshape(external_base_scores[0],\n",
    "#                                                                (np.shape(external_base_scores[0])[0],1))),\n",
    "#                                          axis=1)\n",
    "#                 Xt_test= np.concatenate( (Xt_test,np.reshape(external_base_scores[1],\n",
    "#                                                             (np.shape(external_base_scores[1])[0],1))), \n",
    "#                                         axis=1)\n",
    "            #\n",
    "            if self.saveInternalVectors:\n",
    "                fname= 'STACK_internal_train_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_train)\n",
    "                fname= 'STACK_internal_test_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_test)\n",
    "            i+=1\n",
    "        Xt_train.drop('target',axis=1,inplace=True)\n",
    "        self.top_layer.fit(Xt_train.as_matrix(),y)\n",
    "        return self.top_layer.predict_proba(Xt_test.as_matrix())[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now specify the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now score the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Read data\n",
    "\n",
    "# # train\n",
    "# X_train= np.load(open('../WP014/dummy_train_matrix.bin','rb'))\n",
    "# y_train= np.load(open('../WP014/dummy_train_labels.bin','rb'))\n",
    "\n",
    "# # test\n",
    "# X_test= np.load(open('../WP014/dummy_test_matrix.bin','rb'))\n",
    "# y_test= np.load(open('../WP014/dummy_test_labels.bin','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection, pipe0\n",
    "\n",
    "df_train= pd.read_csv('../data/train.csv')\n",
    "df_test= pd.read_csv('../data/test.csv')\n",
    "\n",
    "index_train, index_test= sklearn.model_selection.train_test_split( range(len(df_train.index)) , \n",
    "                                                                    test_size=0.3,random_state=1)\n",
    "\n",
    "df_test= df_train.loc[index_test,:].reset_index(drop=True)\n",
    "y_test= df_test.target.values\n",
    "df_train= df_train.loc[index_train,:].reset_index(drop=True)\n",
    "y_train= df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read rgf pipeline train and test scores\n",
    "# X_rgf_train= pd.read_csv('../wp017/rgf_scores_train.csv').target.values\n",
    "# X_rgf_test= pd.read_csv('../wp017/rgf_blind_scores.csv').target.values\n",
    "X_rgf_train= pd.read_csv('../wp018/output_0.01_300leaf/rgf_validation_ scores.csv')\n",
    "X_rgf_test= X_rgf_train.target.values[index_test]\n",
    "X_rgf_train= X_rgf_train.target.values[index_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_ids= pd.read_csv('../data/test.csv',usecols=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm.sklearn\n",
    "import xgboost.sklearn\n",
    "import catboost\n",
    "import sklearn.linear_model, sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.01\n",
    "lgb_params['n_estimators'] = 1300\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['num_leaves']= 25\n",
    "lgb_params['n_jobs']=8\n",
    "\n",
    "\n",
    "# lgb_params_3 = {\n",
    "#     'learning_rate': 0.02,\n",
    "#     'n_estimators': 800,\n",
    "#     'max_depth': 4,\n",
    "#     'n_jobs':8\n",
    "# }\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 800, #150,\n",
    "    'max_depth': 4,\n",
    "    'n_jobs':8\n",
    "#     'min_child_samples':100\n",
    "}\n",
    "\n",
    "lgb_params_4 = {\n",
    "    'learning_rate':0.05,\n",
    "    'n_estimators':600,\n",
    "    'num_leaves':35,\n",
    "    'min_child_samples':500,\n",
    "    'n_jobs':8\n",
    "}\n",
    "\n",
    "\n",
    "xgb_params= {'learning_rate': 0.07,\n",
    "             'n_estimators':525,\n",
    "             'max_depth': 4, \n",
    "             'nthread':8,\n",
    "             'subsample': 0.8,\n",
    "             'min_child_weight':6.0, \n",
    "             'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', \n",
    "             'eval_metric': 'auc', \n",
    "             'seed': 99, \n",
    "             'silent': True,\n",
    "             'scale_pos_weight': 1.6,\n",
    "             'reg_alpha':8,\n",
    "             'reg_lambda':1.3,\n",
    "             'gamma':10\n",
    "            }\n",
    "\n",
    "cb_params= {\n",
    "    'learning_rate':0.05, \n",
    "    'depth':6, \n",
    "    'l2_leaf_reg': 14, \n",
    "    'iterations': 650,\n",
    "    'verbose': False,\n",
    "    'loss_function':'Logloss'\n",
    "    }\n",
    "\n",
    "# lgbmn_params= {'num_leaves': 81, 'verbose': 1, 'learning_rate': 0.005, \n",
    "#                'min_data': 650, 'categorical_column': [], 'bagging_fraction': 0.9, \n",
    "#                'metric': ['auc'], 'boosting_type': 'gbdt', 'lambda_l1': 30,\n",
    "#                'bagging_freq': 3, 'lambda_l2': 0, 'is_unbalance': True, \n",
    "#                'max_bin': 255, 'objective': ['binary'], 'max_depth': 6, \n",
    "#                'feature_fraction': 0.7,'n_estimators':1600\n",
    "#               }\n",
    "\n",
    "# Layer 1\n",
    "lgbm1 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params))\n",
    "xgbm1= (pipe0.run_pipe0,   xgboost.sklearn.XGBClassifier(**xgb_params))\n",
    "lgbm3 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params_3))\n",
    "lgbm4 = (pipe0.run_pipe0,  lightgbm.sklearn.LGBMClassifier(**lgb_params_4))\n",
    "cb= (pipe0.run_pipe0,      catboost.CatBoostClassifier(**cb_params))\n",
    "# lgbmn = (pipe3.run_pipe3,  lightgbm.sklearn.LGBMClassifier(**lgbmn_params))\n",
    "\n",
    "# Top layer\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.0,class_weight='balanced',penalty='l1')\n",
    "\n",
    "# Define the stack\n",
    "stack = Stack(3,[ [cb,xgbm1,lgbm1,lgbm3,lgbm4] ], stacker, saveInternalVectors=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stack layer 1\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x11585c150> fold 1\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x11585c150> fold 2\n",
      "Fit run_pipe0 --> <catboost.core.CatBoostClassifier object at 0x11585c150> fold 3\n",
      "Fit run_pipe0 --> XGBClassifier fold 1\n",
      "Fit run_pipe0 --> XGBClassifier fold 2\n",
      "Fit run_pipe0 --> XGBClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      "Fit run_pipe0 --> LGBMClassifier fold 1\n",
      "Fit run_pipe0 --> LGBMClassifier fold 2\n",
      "Fit run_pipe0 --> LGBMClassifier fold 3\n",
      " Training took 1126.947799\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "from datetime import datetime\n",
    "tc= datetime.now()\n",
    "y_pred = stack.fit_predict(df_train, df_train.target.values, df_test,\n",
    "                            external_base_scores= (X_rgf_train, X_rgf_test) ) #,\n",
    "print' Training took '+str( (datetime.now() - tc).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Gini score=  0.291740847787 0.290426135503\n"
     ]
    }
   ],
   "source": [
    "print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1.,0.290426135503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l2')\n",
    "X_train= np.load('STACK_internal_train_layer_1.bin')\n",
    "X_test= np.load('STACK_internal_test_layer_1.bin')\n",
    "y_train= X_train[:,5]\n",
    "X_train= np.delete(X_train,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sklearn.preprocessing\n",
    "# fvg= sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "# X_train_pol= fvg.fit_transform(X_train)\n",
    "# X_test_pol= fvg.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Gini score=  0.286463134292 0.282201168818\n"
     ]
    }
   ],
   "source": [
    "#  This for testing ensemble incl RGF\n",
    "internal= [0,1,2,3,4,5]\n",
    "folds = StratifiedKFold(y_train,n_folds=10, shuffle=True, random_state=2016)\n",
    "y_pred= np.zeros(len(y_train))\n",
    "for i, (itr,ite) in enumerate(folds):\n",
    "    stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l1')\n",
    "    stacker.fit( X_train[np.ix_(itr,internal)] , y_train[itr] )\n",
    "    y_pred[ite]= stacker.predict_proba(X_train[np.ix_(ite,internal)])[:,1]\n",
    "print 'CV Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train,y_pred)-1.,0.282201168818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try the bagging approach (which will take longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took:  42.367401\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "from datetime import datetime,timedelta\n",
    "tc= datetime.now()\n",
    "# lgb_params_3 = {\n",
    "#     'learning_rate': 0.1,\n",
    "#     'n_estimators': 115,\n",
    "#     'max_depth': 1,\n",
    "#     'n_jobs':1\n",
    "# #     'eval_metric':'auc'\n",
    "# }\n",
    "# altStacker= xgboost.sklearn.XGBClassifier(**lgb_params_3)\n",
    "clf= sklearn.ensemble.BaggingClassifier(stacker,\n",
    "                                        n_estimators=128,\n",
    "                                        oob_score=True,\n",
    "                                        max_features=4,\n",
    "                                        max_samples=0.6,\n",
    "                                        random_state=1,\n",
    "                                        n_jobs=-1\n",
    "                                       )\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred= clf.predict_proba(X_test)[:,1]\n",
    "print 'Training took: ',(datetime.now() - tc).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416648 416648\n"
     ]
    }
   ],
   "source": [
    "y_pred2= clf.oob_decision_function_[:,1]\n",
    "ok= np.where(map(np.isfinite,y_pred2))[0]\n",
    "print len(ok),len(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Gini score=  0.286746342768 0.286463220363 (0.286760096353)\n",
      "Test Gini score=  0.292071239454 0.291740847787 (0.292343541304)\n"
     ]
    }
   ],
   "source": [
    "print 'OOB Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train[ok],y_pred2[ok])-1.,0.286463220363,'(0.286760096353)'\n",
    "print 'Test Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1.,0.291740847787,'(0.292343541304)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now explore performance an ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l2')\n",
    "X_train= np.load('STACK_internal_train_layer_1.bin')#[:,[0,1,2,3,4,6]]\n",
    "X_test= np.load('STACK_internal_test_layer_1.bin')#[:,[0,1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0 Gini score=  0.288206437232\n",
      "0.05 0.95 Gini score=  0.289637119888\n",
      "0.1 0.9 Gini score=  0.290377447112\n",
      "0.15 0.85 Gini score=  0.290774297193\n",
      "0.2 0.8 Gini score=  0.29097533394\n",
      "0.25 0.75 Gini score=  0.291072110783\n",
      "0.3 0.7 Gini score=  0.291104164363\n",
      "0.35 0.65 Gini score=  0.291103406357\n",
      "0.4 0.6 Gini score=  0.291081280496\n",
      "0.45 0.55 Gini score=  0.291042600653\n",
      "0.5 0.5 Gini score=  0.291001653979\n",
      "0.55 0.45 Gini score=  0.290952681785\n",
      "0.6 0.4 Gini score=  0.290905918945\n",
      "0.65 0.35 Gini score=  0.29085518825\n",
      "0.7 0.3 Gini score=  0.290807232717\n",
      "0.75 0.25 Gini score=  0.290761906854\n",
      "0.8 0.2 Gini score=  0.2907136765\n",
      "0.85 0.15 Gini score=  0.290667059153\n",
      "0.9 0.1 Gini score=  0.290626998016\n",
      "0.95 0.05 Gini score=  0.290586058527\n",
      "1.0 0.0 Gini score=  0.290544844215\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "    y_pred2 = ((i/20.)*y_pred + (1.-i/20.)*X_test[:,5])\n",
    "    print (i/20.),(1.-i/20.),'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred2)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train= X_train[:,5]\n",
    "X_train= np.delete(X_train,5,1)\n",
    "# X_train= np.vstack( (X_train,X_test) )\n",
    "# y_train= np.concatenate( (y_train,y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini score=  0.289558478616 0.284310686184\n"
     ]
    }
   ],
   "source": [
    "#  This for testing ensemble before RGF\n",
    "internal= [0,1,2,3,4]\n",
    "folds = StratifiedKFold(y_train,n_folds=10, shuffle=True, random_state=2016)\n",
    "y_pred= np.zeros(len(y_train))\n",
    "for i, (itr,ite) in enumerate(folds):\n",
    "    stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l1')\n",
    "    stacker.fit( X_train[np.ix_(itr,internal)] , y_train[itr] )\n",
    "    y_pred[ite]= stacker.predict_proba(X_train[np.ix_(ite,internal)])[:,1]\n",
    "print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train,y_pred)-1.,0.284310686184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000 1.0000 Gini score=  0.285007751451\n",
      "0.0500 0.9500 Gini score=  0.287101445587\n",
      "0.1000 0.9000 Gini score=  0.288182105659\n",
      "0.1500 0.8500 Gini score=  0.28878560097\n",
      "0.2000 0.8000 Gini score=  0.28913908976\n",
      "0.2500 0.7500 Gini score=  0.289356094245\n",
      "0.3000 0.7000 Gini score=  0.289487127775\n",
      "0.3500 0.6500 Gini score=  0.289569706323\n",
      "0.4000 0.6000 Gini score=  0.289619093477\n",
      "0.4500 0.5500 Gini score=  0.289646469361\n",
      "0.5000 0.5000 Gini score=  0.289659490684\n",
      "0.5500 0.4500 Gini score=  0.289661597276\n",
      "0.6000 0.4000 Gini score=  0.289658667498\n",
      "0.6500 0.3500 Gini score=  0.289651731419\n",
      "0.7000 0.3000 Gini score=  0.289641535227\n",
      "0.7500 0.2500 Gini score=  0.289629455238\n",
      "0.8000 0.2000 Gini score=  0.289616606557\n",
      "0.8500 0.1500 Gini score=  0.289602288004\n",
      "0.9000 0.1000 Gini score=  0.28958784455\n",
      "0.9500 0.0500 Gini score=  0.289572977528\n",
      "1.0000 0.0000 Gini score=  0.289558478616\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "    y_pred3 = ((i/20.)*y_pred + (1.-i/20.)*X_train[:,5])\n",
    "    print '%.4f %.4f'% ((i/20.),(1.-i/20.)),'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train,y_pred3)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe0\n",
      "0 Gini score=  0.289429121198  weights: [[ 0.78289255  2.42900245  3.92418095]]\n",
      "1 Gini score=  0.289436484221  weights: [[ 0.92937558  4.96545468  3.90022289]]\n",
      "2 Gini score=  0.28890543862  weights: [[ 1.12619707  2.06088579  3.93985128]]\n",
      "3 Gini score=  0.289621946739  weights: [[ 3.06923959  2.17780009  4.72924818]]\n",
      "4 Gini score=  0.289443640846  weights: [[ 0.59466449  1.17623999  4.18440041]]\n"
     ]
    }
   ],
   "source": [
    "print 'pipe0'\n",
    "for other_external_candidate in range(5):\n",
    "#     other_external_candidate=1\n",
    "    internal= [0,1,2,3,4]\n",
    "    internal.remove(other_external_candidate)\n",
    "    external= [other_external_candidate,5]\n",
    "\n",
    "    folds = StratifiedKFold(y_train,n_folds=10, shuffle=True, random_state=2016)\n",
    "    y_pred= np.zeros(len(y_train))\n",
    "    # y_test_pred_i= np.zeros([len(y_test),10])\n",
    "    for i, (itr,ite) in enumerate(folds):\n",
    "        stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l1')\n",
    "        stacker.fit( X_train[np.ix_(itr,internal)] , y_train[itr] )\n",
    "        y_pred[ite]= stacker.predict_proba(X_train[np.ix_(ite,internal)])[:,1]\n",
    "    #     y_test_pred_i[:,i]= stacker.predict_proba(X_test[:,internal])[:,1]\n",
    "    # y_test_pred= y_test_pred_i.mean(axis=1)\n",
    "\n",
    "    folds = StratifiedKFold(y_train,n_folds=10, shuffle=True, random_state=2016)\n",
    "    y_pred2= np.zeros( len(y_train) )\n",
    "    for i, (itr,ite) in enumerate(folds):\n",
    "        stacker2= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l1')\n",
    "        ddtr_dict= {'x1':y_pred[itr]}\n",
    "    #     ddte_dict= {'x1':y_test_pred}\n",
    "        ddte_dict={'x1':y_pred[ite]}\n",
    "        for ii in external:\n",
    "            ddtr_dict[str(ii)]= X_train[itr,ii]\n",
    "    #         ddte_dict[str(ii)]= X_test[:,ii]\n",
    "            ddte_dict[str(ii)]= X_train[ite,ii]\n",
    "        stacker2.fit( pd.DataFrame(ddtr_dict).as_matrix(), y_train[itr])\n",
    "        y_pred2[ite]= stacker2.predict_proba( pd.DataFrame(ddte_dict).as_matrix() )[:,1]\n",
    "    print other_external_candidate,'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_train,y_pred2)-1.,' weights:',np.abs(stacker2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full run\n",
      "0 Gini score=  0.289429121198  weights: [[ 0.78289255  2.42900245  3.92418095]]\n",
      "1 Gini score=  0.289436484221  weights: [[ 0.92937558  4.96545468  3.90022289]]\n",
      "2 Gini score=  0.28890543862  weights: [[ 1.12619707  2.06088579  3.93985128]]\n",
      "3 Gini score=  0.289621946739  weights: [[ 3.06923959  2.17780009  4.72924818]]\n",
      "4 Gini score=  0.289443640846  weights: [[ 0.59466449  1.17623999  4.18440041]]\n"
     ]
    }
   ],
   "source": [
    "print 'full run'\n",
    "print '0 Gini score=  0.289429121198  weights: [[ 0.78289255  2.42900245  3.92418095]]'\n",
    "print '1 Gini score=  0.289436484221  weights: [[ 0.92937558  4.96545468  3.90022289]]'\n",
    "print '2 Gini score=  0.28890543862  weights: [[ 1.12619707  2.06088579  3.93985128]]'\n",
    "print '3 Gini score=  0.289621946739  weights: [[ 3.06923959  2.17780009  4.72924818]]'\n",
    "print '4 Gini score=  0.289443640846  weights: [[ 0.59466449  1.17623999  4.18440041]]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe1\n",
      "Original ensemble Gini score= 0.284310686184\n",
      "Leave 1 out recent pipe ensemble with RGF\n",
      "[]Gini score=  0.286829819859  weights: [[ 11.13969069,   2.41252981]]\n",
      "0 Gini score=  0.286756089638  weights: [[  1.27887833  11.85710495   2.52665091]]\n",
      "1 Gini score=  0.286975202645  weights: [[  4.32187093  14.61451641   3.0597565 ]]\n",
      "2 Gini score=  0.286169060463  weights: [[  0.20000129  12.49432133   2.1111492 ]]\n",
      "3 Gini score=  0.286687060147  weights: [[  3.58953716  11.51256105   3.04540583]]\n",
      "4 Gini score=  0.286686377293  weights: [[  0.50350095  11.0010589    2.54338766]]\n",
      "-------------------------\n",
      "pipe0\n",
      "Original ensemble Gini score= 0.282336875599\n",
      "Leave 1 out recent pipe ensemble with RGF\n",
      "[]Gini score=  0.286964611496\n",
      "0 Gini score=  0.286785357999  weights: [[  0.32715968  11.10372353   2.48246342]]\n",
      "1 Gini score=  0.287110187178  weights: [[  3.88722331  14.11573311   3.01882178]]\n",
      "2 Gini score=  0.286332729041  weights: [[  0.84960471  11.3754384    2.53265623]]\n",
      "3 Gini score=  0.286820601653  weights: [[  3.73058137  10.83776478   3.19750962]]\n",
      "4 Gini score=  0.286785410724  weights: [[  0.78280637  10.20362933   2.75108272]]\n"
     ]
    }
   ],
   "source": [
    "print 'pipe1'\n",
    "print \"Original ensemble Gini score= 0.284310686184\"\n",
    "print \"Leave 1 out recent pipe ensemble with RGF\"\n",
    "print \"[]Gini score=  0.286829819859  weights: [[ 11.13969069,   2.41252981]]\"\n",
    "print \"0 Gini score=  0.286756089638  weights: [[  1.27887833  11.85710495   2.52665091]]\"\n",
    "print \"1 Gini score=  0.286975202645  weights: [[  4.32187093  14.61451641   3.0597565 ]]\"\n",
    "print \"2 Gini score=  0.286169060463  weights: [[  0.20000129  12.49432133   2.1111492 ]]\"\n",
    "print \"3 Gini score=  0.286687060147  weights: [[  3.58953716  11.51256105   3.04540583]]\"\n",
    "print \"4 Gini score=  0.286686377293  weights: [[  0.50350095  11.0010589    2.54338766]]\"\n",
    "print \"-------------------------\"\n",
    "print \"pipe0\"\n",
    "print \"Original ensemble Gini score= 0.282336875599\"\n",
    "print \"Leave 1 out recent pipe ensemble with RGF\"\n",
    "print \"[]Gini score=  0.286964611496\"\n",
    "print \"0 Gini score=  0.286785357999  weights: [[  0.32715968  11.10372353   2.48246342]]\"\n",
    "print \"1 Gini score=  0.287110187178  weights: [[  3.88722331  14.11573311   3.01882178]]\"\n",
    "print \"2 Gini score=  0.286332729041  weights: [[  0.84960471  11.3754384    2.53265623]]\"\n",
    "print \"3 Gini score=  0.286820601653  weights: [[  3.73058137  10.83776478   3.19750962]]\"\n",
    "print \"4 Gini score=  0.286785410724  weights: [[  0.78280637  10.20362933   2.75108272]]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blind test\n"
     ]
    }
   ],
   "source": [
    "print 'blind test'\n",
    "# for other_external_candidate in range(5):\n",
    "other_external_candidate=1\n",
    "internal= [0,1,2,3,4]\n",
    "internal.remove(other_external_candidate)\n",
    "external= [other_external_candidate,5]\n",
    "\n",
    "folds = StratifiedKFold(y_train,n_folds=5, shuffle=True, random_state=2017)\n",
    "y_pred= np.zeros(len(y_train))\n",
    "y_test_pred_i= np.zeros([np.shape(X_test)[0],5])\n",
    "for i, (itr,ite) in enumerate(folds):\n",
    "    stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l1')\n",
    "    stacker.fit( X_train[np.ix_(itr,internal)] , y_train[itr] )\n",
    "    y_pred[ite]= stacker.predict_proba(X_train[np.ix_(ite,internal)])[:,1]\n",
    "    y_test_pred_i[:,i]= stacker.predict_proba(X_test[:,internal])[:,1]\n",
    "y_test_pred= y_test_pred_i.mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "stacker2= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l1')\n",
    "ddtr_dict= {'x1':y_pred}\n",
    "ddte_dict= {'x1':y_test_pred}\n",
    "for ii in external:\n",
    "    ddtr_dict[str(ii)]= X_train[:,ii]\n",
    "    ddte_dict[str(ii)]= X_test[:,ii]\n",
    "stacker2.fit( pd.DataFrame(ddtr_dict).as_matrix(), y_train)\n",
    "y_pred2= stacker2.predict_proba( pd.DataFrame(ddte_dict).as_matrix() )[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ORIGINAL MODELS TEST:\n",
    "import sklearn.linear_model\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l2')\n",
    "X_train= np.load('STACK_internal_train_layer_1.bin')#[:,[0,1,2,3,4,6]]\n",
    "X_test= np.load('STACK_internal_test_layer_1.bin')#[:,[0,1,2,3,4,5]]\n",
    "\n",
    "y_train= X_train[:,5]\n",
    "X_train= np.delete(X_train,5,1)\n",
    "\n",
    "#  This for testing ensemble before RGF\n",
    "internal= [0,1,2,3,4]\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=500.,class_weight='balanced',penalty='l1')\n",
    "stacker.fit( X_train[:,internal] , y_train )\n",
    "y_pred2= stacker.predict_proba(X_test[:,internal])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optionally average with rgf\n",
    "y_pred3= 0.85*y_pred2 + 0.15 * X_test[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_df= pd.DataFrame( {'id': test_ids.id.values, 'target': y_pred3},\n",
    "                     columns=['id','target']\n",
    "                    ).to_csv('submission_wp016b_averagedStackRGF_0.85_0.15.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_params= {'learning_rate': 0.07,\n",
    "             'n_estimators':1000, #525, #,354\n",
    "             'max_depth': 4, \n",
    "             'nthread':8,\n",
    "             'subsample': 0.8,\n",
    "             'min_child_weight':0.77,\n",
    "             'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', \n",
    "             'eval_metric': 'auc', \n",
    "             'seed': 99, \n",
    "             'silent': True,\n",
    "             'scale_pos_weight': 1.6,\n",
    "             'reg_alpha':8,\n",
    "             'reg_lambda':1.3,\n",
    "             'gamma':10\n",
    "            }\n",
    "xgbm1= xgboost.sklearn.XGBClassifier(**xgb_params)\n",
    "xgbm1.fit(X_train,y_train,eval_metric='auc',early_stopping_rounds=50,eval_set=[(X_test,y_test)])\n",
    "y_pred= xgbm1.predict_proba(X_test)[:,1]\n",
    "print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep= np.where(xgbm1.feature_importances_ > 0.)[0]\n",
    "# keep2= np.where(xgbm1.feature_importances_ > 0.)[0]\n",
    "keeps= keep[keep2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(open('xgboost_rfe_keepers.bin','wb'),keeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 650\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.05\n",
    "cb_params= {\n",
    "    'learning_rate':0.05, \n",
    "    'depth':6, \n",
    "    'l2_leaf_reg': 14, \n",
    "    'iterations':650,\n",
    "    'verbose': True,\n",
    "    'loss_function':'Logloss'\n",
    "    }\n",
    "\n",
    "cb= catboost.CatBoostClassifier(**cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "y_pred= cb.predict_proba(X_test)[:,1]\n",
    "print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection, pipe0\n",
    "\n",
    "df_train= pd.read_csv('../data/train.csv')\n",
    "df_test= pd.read_csv('../data/test.csv')\n",
    "\n",
    "index_train, index_test= sklearn.model_selection.train_test_split( range(len(df_train.index)) , \n",
    "                                                                    test_size=0.3,random_state=1)\n",
    "\n",
    "df_test= df_train.loc[index_test,:].reset_index(drop=True)\n",
    "y_test= df_test.target.values\n",
    "df_train= df_train.loc[index_train,:].reset_index(drop=True)\n",
    "y_train= df_train.target.values\n",
    "\n",
    "X_train,y_train,X_test,y_test= pipe0.run_pipe0(df_train.copy(),df_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took: 1384.537071\n",
      "Gini score=  0.282122576194\n"
     ]
    }
   ],
   "source": [
    "import rgf.sklearn\n",
    "from datetime import datetime, timedelta\n",
    "tc= datetime.now()\n",
    "rgfm= rgf.sklearn.RGFClassifier(max_leaf= 4000,\n",
    "                                test_interval=100,\n",
    "                                loss='Log',\n",
    "                                algorithm='RGF',\n",
    "                                l2=0.01,\n",
    "                                learning_rate=0.5,\n",
    "                                n_jobs=-1,\n",
    "                                opt_interval=100,\n",
    "                                normalize=False,\n",
    "                                min_samples_leaf=10,\n",
    "                                verbose=0)\n",
    "rgfm.fit(X_train,y_train)\n",
    "y_pred= rgfm.predict_proba(X_test)[:,1]\n",
    "print 'Training took:',(datetime.now()-tc).total_seconds()\n",
    "print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-01\n",
      "Gini score=  0.26974399563\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-02\n",
      "Gini score=  0.277497999952\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-03\n",
      "Gini score=  0.280219238402\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-04\n",
      "Gini score=  0.282373059337\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-05\n",
      "Gini score=  0.283301860532\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-06\n",
      "Gini score=  0.285785908779\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-07\n",
      "Gini score=  0.286462742254\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-08\n",
      "Gini score=  0.286310242117\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-09\n",
      "Gini score=  0.285869677384\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-10\n",
      "Gini score=  0.285896839851\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-11\n",
      "Gini score=  0.285754867371\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-12\n",
      "Gini score=  0.285650058721\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-13\n",
      "Gini score=  0.285657690868\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-14\n",
      "Gini score=  0.285178406776\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-15\n",
      "Gini score=  0.284902738806\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-16\n",
      "Gini score=  0.284119886926\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-17\n",
      "Gini score=  0.283357347623\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-18\n",
      "Gini score=  0.282863234821\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-19\n",
      "Gini score=  0.282120599452\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-20\n",
      "Gini score=  0.282121711313\n",
      "/tmp/rgf/beae9929-6209-461a-8edb-f6e34d6a2aac1.model-21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5cef2a1ac29a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrgfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_model_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmystr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%02d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mrgfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_model_loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrgfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_av\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidsutton/anaconda/lib/python2.7/site-packages/rgf/sklearn.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    644\u001b[0m                              % (self._n_features, n_features))\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidsutton/anaconda/lib/python2.7/site-packages/rgf/sklearn.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    820\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidsutton/anaconda/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;31m# Parse each line, including the first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mystr= rgfm._estimators[0]._latest_model_loc\n",
    "for i in range(40):\n",
    "    rgfm._estimators[0]._latest_model_loc = mystr[:-2] + '%02d' % (i+1)\n",
    "    print rgfm._estimators[0]._latest_model_loc\n",
    "    y_pred= rgfm.predict_proba(X_test)[:,1]\n",
    "    if i==0:\n",
    "        y_av= y_pred\n",
    "    else:\n",
    "        y_av += y_pred\n",
    "    print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/rgf/b6c38df2-caa8-406c-9368-dada62a218722.model-20'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgfm.estimators_[0]._latest_model_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan: RGF is slow, so you would want to do the parallelisation of k-folds 'by hand', outputting predictions to binary files, then concatenating them at the end.  You can also use every estimator to form an ensemble.  To use this requires outputting n_estimators x predictions files per fold, concatenating and feeding into a LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import lightgbm\n",
    "# lgb_params_3 = {\n",
    "#     'learning_rate': 0.02,\n",
    "#     'n_estimators': 150,\n",
    "#     'max_depth': 4,\n",
    "#     'n_jobs':8,\n",
    "# #     'early_stopping_round':50,\n",
    "#     'min_child_samples':100\n",
    "# }\n",
    "\n",
    "# lgbmn = lightgbm.sklearn.LGBMClassifier(**lgb_params_3)\n",
    "# bagger= sklearn.ensemble.BaggingClassifier(lgbmn,n_estimators=10,max_samples=0.3,random_state=1234)\n",
    "# bagger.fit(X_train,y_train)\n",
    "# # lgbmn.fit(X_train,y_train,eval_metric='auc',eval_set=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGFClassifier(algorithm='RGF', calc_prob='sigmoid', l2=0.1, learning_rate=0.5,\n",
       "       loss='Log', max_leaf=2000, memory_policy='generous',\n",
       "       min_samples_leaf=10, n_iter=None, n_jobs=-1, n_tree_search=1,\n",
       "       normalize=False, opt_interval=100, reg_depth=1.0, sl2=None,\n",
       "       test_interval=100, verbose=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "y_pred= bagger.predict_proba(X_test)[:,1]\n",
    "print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.289135174661"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RGFClassifier in module rgf.sklearn:\n",
      "\n",
      "class RGFClassifier(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin)\n",
      " |  A Regularized Greedy Forest [1] classifier.\n",
      " |  \n",
      " |  Tuning parameters detailed instruction:\n",
      " |      https://github.com/fukatani/rgf_python/blob/master/include/rgf/rgf1.2-guide.pdf\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  max_leaf : int, optional (default=1000)\n",
      " |      Training will be terminated when the number of\n",
      " |      leaf nodes in the forest reaches this value.\n",
      " |  \n",
      " |  test_interval : int, optional (default=100)\n",
      " |      Test interval in terms of the number of leaf nodes.\n",
      " |  \n",
      " |  algorithm : string (\"RGF\" or \"RGF_Opt\" or \"RGF_Sib\"), optional (default=\"RGF\")\n",
      " |      Regularization algorithm.\n",
      " |      RGF: RGF with L2 regularization on leaf-only models.\n",
      " |      RGF Opt: RGF with min-penalty regularization.\n",
      " |      RGF Sib: RGF with min-penalty regularization with the sum-to-zero sibling constraints.\n",
      " |  \n",
      " |  loss : string (\"LS\" or \"Expo\" or \"Log\"), optional (default=\"Log\")\n",
      " |      Loss function.\n",
      " |      LS: Square loss.\n",
      " |      Expo: Exponential loss.\n",
      " |      Log: Logistic loss.\n",
      " |  \n",
      " |  reg_depth : float, optional (default=1.0)\n",
      " |      Must be no smaller than 1.0.\n",
      " |      Meant for being used with algorithm=\"RGF Opt\"|\"RGF Sib\".\n",
      " |      A larger value penalizes deeper nodes more severely.\n",
      " |  \n",
      " |  l2 : float, optional (default=0.1)\n",
      " |      Used to control the degree of L2 regularization.\n",
      " |  \n",
      " |  sl2 : float or None, optional (default=None)\n",
      " |      Override L2 regularization parameter l2\n",
      " |      for the process of growing the forest.\n",
      " |      That is, if specified, the weight correction process uses l2\n",
      " |      and the forest growing process uses sl2.\n",
      " |      If None, no override takes place and\n",
      " |      l2 is used throughout training.\n",
      " |  \n",
      " |  normalize : boolean, optional (default=False)\n",
      " |      If True, training targets are normalized\n",
      " |      so that the average becomes zero.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, optional (default=10)\n",
      " |      Minimum number of training data points in each leaf node.\n",
      " |      If int, then consider min_samples_leaf as the minimum number.\n",
      " |      If float, then min_samples_leaf is a percentage and\n",
      " |      ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
      " |  \n",
      " |  n_iter : int or None, optional (default=None)\n",
      " |      Number of iterations of coordinate descent to optimize weights.\n",
      " |      If None, 10 is used for loss=\"LS\" and 5 for loss=\"Expo\"|\"Log\".\n",
      " |  \n",
      " |  n_tree_search : int, optional (default=1)\n",
      " |      Number of trees to be searched for the nodes to split.\n",
      " |      The most recently grown trees are searched first.\n",
      " |  \n",
      " |  opt_interval : int, optional (default=100)\n",
      " |      Weight optimization interval in terms of the number of leaf nodes.\n",
      " |      For example, by default, weight optimization is performed\n",
      " |      every time approximately 100 leaf nodes are newly added to the forest.\n",
      " |  \n",
      " |  learning_rate : float, optional (default=0.5)\n",
      " |      Step size of Newton updates used in coordinate descent to optimize weights.\n",
      " |  \n",
      " |  calc_prob : string (\"sigmoid\" or \"softmax\"), optional (default=\"sigmoid\")\n",
      " |      Method of probability calculation.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=-1)\n",
      " |      The number of jobs to use for the computation.\n",
      " |      If 1 is given, no parallel computing code is used at all.\n",
      " |      If -1 all CPUs are used.\n",
      " |      For n_jobs = -2, all CPUs but one are used.\n",
      " |      For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.\n",
      " |  \n",
      " |  memory_policy : string (\"conservative\" or \"generous\"), optional (default=\"generous\")\n",
      " |      Memory using policy.\n",
      " |      Generous: it runs faster using more memory by keeping the sorted orders\n",
      " |      of the features on memory for reuse.\n",
      " |      Conservative: it uses less memory at the expense of longer runtime. Try only when\n",
      " |      with default value it uses too much memory.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  Attributes:\n",
      " |  -----------\n",
      " |  estimators_ : list of binary classifiers\n",
      " |      The collection of fitted sub-estimators when `fit` is performed.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes]\n",
      " |      The classes labels when `fit` is performed.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes when `fit` is performed.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when `fit` is performed.\n",
      " |  \n",
      " |  fitted_ : boolean\n",
      " |      Indicates whether `fit` is performed.\n",
      " |  \n",
      " |  sl2_ : float\n",
      " |      The concrete regularization value for the process of growing the forest\n",
      " |      used in model building process.\n",
      " |  \n",
      " |  min_samples_leaf_ : int\n",
      " |      Minimum number of training data points in each leaf node\n",
      " |      used in model building process.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iterations of coordinate descent to optimize weights\n",
      " |      used in model building process depending on the specified loss function.\n",
      " |  \n",
      " |  Reference\n",
      " |  ---------\n",
      " |  [1] Rie Johnson and Tong Zhang,\n",
      " |      Learning Nonlinear Functions Using Regularized Greedy Forest.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RGFClassifier\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_leaf=1000, test_interval=100, algorithm='RGF', loss='Log', reg_depth=1.0, l2=0.1, sl2=None, normalize=False, min_samples_leaf=10, n_iter=None, n_tree_search=1, opt_interval=100, learning_rate=0.5, calc_prob='sigmoid', n_jobs=-1, memory_policy='generous', verbose=0)\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a RGF Classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          The target values (class labels in classification).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes].\n",
      " |          The class probabilities of the input samples.\n",
      " |          The order of the classes corresponds to that in the attribute classes_.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |      The classes labels when `fit` is performed.\n",
      " |  \n",
      " |  estimators_\n",
      " |      The collection of fitted sub-estimators when `fit` is performed.\n",
      " |  \n",
      " |  fitted_\n",
      " |      Indicates whether `fit` is performed.\n",
      " |  \n",
      " |  min_samples_leaf_\n",
      " |      Minimum number of training data points in each leaf node\n",
      " |      used in model building process.\n",
      " |  \n",
      " |  n_classes_\n",
      " |      The number of classes when `fit` is performed.\n",
      " |  \n",
      " |  n_features_\n",
      " |      The number of features when `fit` is performed.\n",
      " |  \n",
      " |  n_iter_\n",
      " |      Number of iterations of coordinate descent to optimize weights\n",
      " |      used in model building process depending on the specified loss function.\n",
      " |  \n",
      " |  sl2_\n",
      " |      The concrete regularization value for the process of growing the forest\n",
      " |      used in model building process.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rgf.sklearn\n",
    "help(rgf.sklearn.RGFClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a= pd.read_csv('submission_wp016a.csv')\n",
    "b= pd.read_csv('../wp015/gpx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c= 0.*a.target.values + 1.0*b.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d= pd.DataFrame({'id':a.id.values, 'target':c},columns=['id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.to_csv('submission_gpx_check.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
