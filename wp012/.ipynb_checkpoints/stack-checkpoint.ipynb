{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidsutton/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd,matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(array):\n",
    "    srtInd = array.argsort()\n",
    "    ranks = np.empty(len(array), float)\n",
    "    ranks[srtInd] = np.arange(len(array))\n",
    "    return ranks / float(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, n_splits, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]              \n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                #\n",
    "                test_probs= clf.predict_proba(T)[:,1]\n",
    "                S_test_i[:, j] = np.log(test_probs) - np.log(1.0 - test_probs)\n",
    "            agg_lor= S_test_i.mean(axis=1)\n",
    "            S_test[:, i] = 1.0 / (1.0 + np.exp( -agg_lor) )\n",
    "        return S_train,S_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add output of layer results ofr quick iteration\n",
    "class Stack:\n",
    "    def __init__(self,k_folds,hidden_layers,top_layer,saveInternalVectors=False):\n",
    "        self.saveInternalVectors= saveInternalVectors\n",
    "        self.layers= []\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append( Layer(k_folds,h) )\n",
    "        self.top_layer= top_layer \n",
    "        return None\n",
    "    \n",
    "    def fit_predict(self,X,y,T,external_base_scores= None):\n",
    "        Xt_train= copy.deepcopy(X)\n",
    "        Xt_test= copy.deepcopy(T)\n",
    "        i= 1\n",
    "        for layer in self.layers:\n",
    "            print 'Fitting stack layer '+str(i)\n",
    "            Xt_train, Xt_test= layer.fit_predict(Xt_train,y,Xt_test)\n",
    "            if external_base_scores is not None and i==1:\n",
    "                Xt_train= np.concatenate( (Xt_train,np.reshape(external_base_scores[0],\n",
    "                                                               (np.shape(external_base_scores[0])[0],1))),\n",
    "                                         axis=1)\n",
    "                Xt_test= np.concatenate( (Xt_test,np.reshape(external_base_scores[1],\n",
    "                                                            (np.shape(external_base_scores[1])[0],1))), \n",
    "                                        axis=1)\n",
    "            #\n",
    "            # Add entropy score from layer\n",
    "#             train_entropy= np.array(map(lambda i: np.sum(Xt_train[i] * np.log(Xt_train[i])), \n",
    "#                                         range(np.shape(Xt_train)[0] )))\n",
    "#             Xt_train= np.concatenate( (Xt_train,np.reshape(train_entropy,(len(train_entropy),1))) ,axis=1)\n",
    "#             test_entropy= np.array(map(lambda i: np.sum(Xt_test[i] * np.log(Xt_test[i])), \n",
    "#                                         range(np.shape(Xt_test)[0] )))\n",
    "#             Xt_test= np.concatenate( (Xt_test,np.reshape(test_entropy,(len(test_entropy),1))) ,axis=1)\n",
    "            #\n",
    "#             # Rank transform\n",
    "#             for jj in range(np.shape(Xt_train)[1]):\n",
    "#                 Xt_train[:,jj]= rank(Xt_train[:,jj])\n",
    "#                 Xt_test[:,jj]= rank(Xt_test[:,jj])\n",
    "#             #\n",
    "            if self.saveInternalVectors:\n",
    "                fname= 'STACK_internal_train_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_train)\n",
    "                fname= 'STACK_internal_test_layer_'+str(i)+'.bin'\n",
    "                np.save(open(fname,'wb'),Xt_test)\n",
    "            i+=1\n",
    "#         for i in range(np.shape(Xt_train)[1]): #-1 so we don't apply to entropy!!\n",
    "#             p= copy.deepcopy(Xt_train[:,i])\n",
    "#             Xt_train[:,i]= np.log( p ) - np.log(1.0 - p)\n",
    "        self.top_layer.fit(Xt_train,y)\n",
    "        return self.top_layer.predict_proba(Xt_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now specify the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now score the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "# train\n",
    "X_train= np.load(open('../wp013/full_train_matrix.bin','rb'))\n",
    "y_train= np.load(open('../wp013/full_train_labels.bin','rb'))\n",
    "\n",
    "# test\n",
    "X_test= np.load(open('../wp013/blind_test_matrix.bin','rb'))\n",
    "y_test= np.load(open('../wp013/blind_test_labels.bin','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ids= pd.read_csv('../data/test.csv',usecols=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zz_train= np.load(open('train.bin','rb')) #This is the aggregation over all GP features\n",
    "zz_test= pd.read_csv('gpari.csv').loc[:,'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sklearn.model_selection\n",
    "# X_tr,X_te, zz_tr, zz_te, y_tr, y_te= sklearn.model_selection.train_test_split( X_train, zz_train, y_train, \n",
    "#                                                                               test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_tr2= np.zeros( (np.shape(X_tr)[0],np.shape(X_tr)[1]+1), dtype= float )\n",
    "# X_tr2[:,0:-1]= X_tr\n",
    "# X_tr2[:,-1]= zz_tr\n",
    "# \n",
    "# X_te2= np.zeros( (np.shape(X_te)[0],np.shape(X_te)[1]+1), dtype= float )\n",
    "# X_te2[:,0:-1]= X_te\n",
    "# X_te2[:,-1]= zz_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm.sklearn\n",
    "import xgboost.sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.01\n",
    "lgb_params['n_estimators'] = 1300\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['num_leaves']= 25\n",
    "lgb_params['n_jobs']=8\n",
    "\n",
    "lgb_params_2 = {\n",
    "    'learning_rate': 0.005,\n",
    "    'n_estimators': 3700,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 2,\n",
    "    'colsample_bytree': 0.3,  \n",
    "    'num_leaves': 16,\n",
    "    'n_jobs':8\n",
    "}\n",
    "\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 4,\n",
    "    'n_jobs':8\n",
    "}\n",
    "\n",
    "lgb_params_4 = {\n",
    "    'learning_rate':0.05,\n",
    "    'n_estimators':600,\n",
    "    'num_leaves':35,\n",
    "    'min_child_samples':500,\n",
    "    'n_jobs':8\n",
    "}\n",
    "\n",
    "\n",
    "xgb_params= {'learning_rate': 0.07,\n",
    "             'n_estimators':525,\n",
    "             'max_depth': 4, \n",
    "             'nthread':8,\n",
    "             'subsample': 0.8,\n",
    "             'min_child_weight':0.77,\n",
    "             'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', \n",
    "             'eval_metric': 'auc', \n",
    "             'seed': 99, \n",
    "             'silent': True,\n",
    "             'scale_pos_weight': 1.6,\n",
    "             'reg_alpha':8,\n",
    "             'reg_lambda':1.3,\n",
    "             'gamma':10\n",
    "            }\n",
    "\n",
    "xgb2_params= {'learning_rate': 0.05,\n",
    "             'n_estimators':900,\n",
    "             'max_depth': 2, \n",
    "             'nthread':8,\n",
    "             'subsample': 0.6,\n",
    "             'min_child_weight': 0.77,\n",
    "             'colsample_bytree': 0.6, \n",
    "             'objective': 'binary:logistic', \n",
    "             'eval_metric': 'auc', \n",
    "             'seed': 99, \n",
    "             'silent': True,\n",
    "             'scale_pos_weight': 1.,\n",
    "             'reg_alpha':0.,\n",
    "             'reg_lambda':3.,\n",
    "             'gamma':10\n",
    "            }\n",
    "\n",
    "# lgb_stumps_params = {\n",
    "#     'learning_rate': 0.02,\n",
    "#     'n_estimators': 12800,\n",
    "#     'max_depth': 1,\n",
    "#     'n_jobs':8\n",
    "# }\n",
    "\n",
    "# Layer 1\n",
    "lgbm1 = lightgbm.sklearn.LGBMClassifier(**lgb_params)\n",
    "xgbm1= xgboost.sklearn.XGBClassifier(**xgb_params)\n",
    "lgbm2 = lightgbm.sklearn.LGBMClassifier(**lgb_params_2)\n",
    "lgbm3 = lightgbm.sklearn.LGBMClassifier(**lgb_params_3)\n",
    "lgbm4 = lightgbm.sklearn.LGBMClassifier(**lgb_params_4)\n",
    "# xgbm2= xgboost.sklearn.XGBClassifier(**xgb2_params)\n",
    "\n",
    "# lgbs= lightgbm.sklearn.LGBMClassifier(**lgb_stumps_params)\n",
    "\n",
    "\n",
    "# import sklearn.neural_network\n",
    "# mlp_params= {\n",
    "#     'hidden_layer_sizes':(20,8), \n",
    "#     'activation':'relu', \n",
    "#     'solver':'adam', \n",
    "#     'alpha':0.1,\n",
    "#     'batch_size':'auto', \n",
    "#     'learning_rate_init':0.02, \n",
    "#     'shuffle':True, \n",
    "#     'tol':0.0001, \n",
    "#     'early_stopping': True,\n",
    "#     'validation_fraction':0.1, \n",
    "#     'beta_1':0.9, \n",
    "#     'beta_2':0.999, \n",
    "#     'epsilon':1e-08\n",
    "# }\n",
    "\n",
    "lgb_params_layer2 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 1,\n",
    "    'n_jobs':8\n",
    "}\n",
    "# lgb_params_layer2 = {\n",
    "#     'learning_rate': 0.002,\n",
    "#     'n_estimators': 3300,\n",
    "#     'max_depth': 1,\n",
    "#     'n_jobs':8\n",
    "# }\n",
    "\n",
    "# Layer 2\n",
    "# mlp= sklearn.neural_network.MLPClassifier(**mlp_params)\n",
    "lgb= lightgbm.sklearn.LGBMClassifier(**lgb_params_layer2)\n",
    "lr= sklearn.linear_model.LogisticRegression(C=500.0,class_weight='balanced',penalty='l1')\n",
    "\n",
    "# Top layer\n",
    "stacker= sklearn.linear_model.LogisticRegression(C=0.0013,class_weight='balanced',penalty='l2')\n",
    "\n",
    "# Define the stack\n",
    "stack = Stack(3,[ [lgbm1,lgbm2,lgbm3,lgbm4,xgbm1], [lgb, lr] ], stacker) \n",
    "# stack = Stack(3,[ [lgbm1,lgbm2,lgbm3,lgbm4,xgbm1, lgbs], [lgb, lr] ], stacker, saveInternalVectors=True)\n",
    "# stack = Stack(3,[ [lgbm3,lgbm4] ], stacker)\n",
    "# stack = Stack(10,[ [lgbm1,lgbm2,lgbm3,lgbm4,xgbm1], [lgb,lr] ], stacker) #, saveInternalVectors=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stack layer 1\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit XGBClassifier fold 1\n",
      "Fit XGBClassifier fold 2\n",
      "Fit XGBClassifier fold 3\n",
      "Fit XGBClassifier fold 1\n",
      "Fit XGBClassifier fold 2\n",
      "Fit XGBClassifier fold 3\n",
      "Fitting stack layer 2\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LogisticRegression fold 1\n",
      "Fit LogisticRegression fold 2\n",
      "Fit LogisticRegression fold 3\n",
      " Training took 761.510992\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "from datetime import datetime\n",
    "tc= datetime.now()\n",
    "# y_pred = stack.fit_predict(X_tr, y_tr, X_te)#,\n",
    "#                            external_base_scores= (zz_tr, zz_te)\n",
    "#                           ) \n",
    "y_pred = stack.fit_predict(X_train, y_train, X_test) #,\n",
    "#                            external_base_scores= (zz_train, zz_test)\n",
    "#                           ) \n",
    "print' Training took '+str( (datetime.now() - tc).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini score=  0.288450117185\n"
     ]
    }
   ],
   "source": [
    "print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best local score=  0.289081111974 0.289504671874\n",
      "Baseline 0.288711072302\n"
     ]
    }
   ],
   "source": [
    "print 'Best local score= ',0.289081111974,0.289504671874\n",
    "print 'Baseline',0.288711072302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# out_df= pd.DataFrame( {'id': test_ids.id.values, 'target': y_pred},\n",
    "#                      columns=['id','target']\n",
    "#                     ).to_csv('submission_wp013b.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini score=  0.289662705273\n"
     ]
    }
   ],
   "source": [
    "lgb_params_layer2 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': 3,\n",
    "    'n_jobs':8\n",
    "}\n",
    "lgb= lightgbm.sklearn.LGBMClassifier(**lgb_params_layer2)\n",
    "lr= sklearn.linear_model.LogisticRegression(C=500.0,class_weight='balanced',penalty='l1')\n",
    "# stacker= sklearn.linear_model.LogisticRegression(C=0.0013,class_weight='balanced',penalty='l2')\n",
    "# stack = Stack(3,[ [ lr] ], stacker)\n",
    "X_train= np.load(open('STACK_internal_train_layer_1.bin','rb'))[:,[0,2,3,4]]\n",
    "X_test= np.load(open('STACK_internal_test_layer_1.bin','rb'))[:,[0,2,3,4]]\n",
    "# y_pred = stack.fit_predict(X_train, y_train, X_test)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred= lr.predict_proba(X_test)[:,1]\n",
    "print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.289612152404"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.289612152404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.289576957232\n"
     ]
    }
   ],
   "source": [
    "print 0.289576957232"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hmmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xtr= np.load('STACK_internal_train_layer_3.bin')\n",
    "# Xte= np.load('STACK_internal_test_layer_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lrc= sklearn.linear_model.LogisticRegression(C=0.0013,class_weight={0:1.0,1:15.0})\n",
    "# lrc.fit(Xtr,y_tr)\n",
    "# y_pred= lrc.predict_proba(Xte)[:,1]\n",
    "# print 'Gini score= ',2.*sklearn.metrics.roc_auc_score(y_te,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.shape(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# upscore= np.load(open('../wp013/upscore_ids.bin','rb'))\n",
    "# out_df= pd.read_csv('submission_wp013b.csv')\n",
    "# out_df.loc[ map(lambda x: x in upscore,out_df.loc[:,'id']), 'target'] += 0.1\n",
    "# out_df.to_csv('submission_wp013b_upscore.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xgb2_params= {'learning_rate': 0.05,\n",
    "#              'n_estimators':2000,\n",
    "#              'max_depth': 2, \n",
    "#              'nthread':8,\n",
    "#              'subsample': 0.6,\n",
    "#              'min_child_weight': 0.77,\n",
    "#              'colsample_bytree': 0.6, \n",
    "#              'objective': 'binary:logistic', \n",
    "#              'eval_metric': 'auc', \n",
    "#              'seed': 99, \n",
    "#              'silent': True,\n",
    "#              'scale_pos_weight': 1.,\n",
    "#              'reg_alpha':0.,\n",
    "#              'reg_lambda':3.,\n",
    "#              'gamma':10\n",
    "#             }\n",
    "# xgbm2= xgboost.sklearn.XGBClassifier(**xgb2_params)\n",
    "# xgbm2.fit(X_train,y_train,eval_metric='logloss',early_stopping_rounds=50,eval_set=[(X_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print 0.151308,581,0.151191,902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
