{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidsutton/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    " \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    " \n",
    "def test_gini():\n",
    "    def fequ(a,b):\n",
    "        return abs( a -b) < 1e-6\n",
    "    def T(a, p, g, n):\n",
    "        assert( fequ(gini(a,p), g) )\n",
    "        assert( fequ(gini_normalized(a,p), n) )\n",
    "    T([1, 2, 3], [10, 20, 30], 0.111111, 1)\n",
    "    T([1, 2, 3], [30, 20, 10], -0.111111, -1)\n",
    "    T([1, 2, 3], [0, 0, 0], -0.111111, -1)\n",
    "    T([3, 2, 1], [0, 0, 0], 0.111111, 1)\n",
    "    T([1, 2, 4, 3], [0, 0, 0, 0], -0.1, -0.8)\n",
    "    T([2, 1, 4, 3], [0, 0, 2, 1], 0.125, 1)\n",
    "    T([0, 20, 40, 0, 10], [40, 40, 10, 5, 5], 0, 0)\n",
    "    T([40, 0, 20, 0, 10], [1000000, 40, 40, 5, 5], 0.171428,\n",
    "       0.6)\n",
    "    T([40, 20, 10, 0, 0], [40, 20, 10, 0, 0], 0.285714, 1)\n",
    "    T([1, 1, 0, 1], [0.86, 0.26, 0.52, 0.32], -0.041666,\n",
    "       -0.333333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train= np.load(open('../wp004/train_matrix.bin','rb'))\n",
    "y_train= np.load(open('../wp004/train_labels.bin','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test= np.load(open('../wp004/test_matrix.bin','rb'))\n",
    "y_test= np.load(open('../wp004/test_labels.bin','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595212"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train) + len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in wp007 feature set and split into train and test\n",
    "# import sklearn.cross_validation\n",
    "# df= pd.read_csv('../data/train.csv',usecols= ['id','target'])\n",
    "# train,test= sklearn.cross_validation.train_test_split(range(df['target'].count()),test_size= 0.33,random_state=0)\n",
    "# y_train= df.target.values[train]\n",
    "# y_test= df.target.values[test]\n",
    "# df2= pd.read_csv('../wp007/masked_full_train.csv')\n",
    "# X_train= df2.as_matrix()[train,:]\n",
    "# X_test= df2.as_matrix()[test,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d83dea889f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df= pd.DataFrame(X_train,columns= ['x'+str(i) for i in range(np.shape(X_train)[1])])\n",
    "# df.loc[:,'y']= y_train\n",
    "# df.to_csv('train.csv',index=False,columns=['y']+['x'+str(i) for i in range(np.shape(X_train)[1])])\n",
    "# \n",
    "# df= pd.DataFrame(X_test,columns= ['x'+str(i) for i in range(np.shape(X_train)[1])])\n",
    "# df.loc[:,'y']= np.zeros(np.shape(X_test)[0])\n",
    "# df.to_csv('test.csv',index=False,columns=['y']+['x'+str(i) for i in range(np.shape(X_train)[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run vanilla random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sklearn.linear_model\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# sample_weights = np.load(open('sample_weights.bin','rb'))\n",
    "\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "#                y_holdout = y[test_idx]\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "#                 if i == len(self.base_models)-1:\n",
    "#                     print 'fitting grey area model'\n",
    "#                     clf.fit(X_train, y_train, sample_weight= sample_weights[train_idx])\n",
    "#                 else:\n",
    "                clf.fit(X_train, y_train)\n",
    "#                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n",
    "        print(\"Stacker score: %.5f\" % (2.*results.mean()-1.))\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        res = self.stacker.predict_proba(S_test)[:,1]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |      Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of boosted trees to fit.\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each split, in each level.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  \n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=None)\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If eval_set is passed to the `fit` function, you can call evals_result() to\n",
      " |      get evaluation results for all passed eval_sets. When eval_metric is also\n",
      " |      passed to the `fit` function, the evals_result will contain the eval_metrics\n",
      " |      passed to the `fit` function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |      clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |      clf.fit(X_train, y_train,\n",
      " |              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |              eval_metric='logloss',\n",
      " |              verbose=True)\n",
      " |      \n",
      " |      evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable evals_result will contain:\n",
      " |      {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |       'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True)\n",
      " |      Fit gradient boosting classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          Weight for each instance\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int, optional\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals.  If there's more than one,\n",
      " |          will use the last. Returns the model from the last iteration\n",
      " |          (not the best one). If early stopping occurs, the model will\n",
      " |          have three additional fields: bst.best_score, bst.best_iteration\n",
      " |          and bst.best_ntree_limit.\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=0)\n",
      " |  \n",
      " |  predict_proba(self, data, output_margin=False, ntree_limit=0)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameter.s\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidsutton/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "help(xgboost.sklearn.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Fit LGBMClassifier fold 1\n",
      "Fit LGBMClassifier fold 2\n",
      "Fit LGBMClassifier fold 3\n",
      "Stacker score: 0.28297\n",
      "Training took:  402.525163  seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tc= datetime.now()\n",
    "import lightgbm.sklearn\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.01\n",
    "lgb_params['n_estimators'] = 1300\n",
    "#lgb_params['max_depth'] = 10\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['num_leaves']= 25\n",
    "# lgb_params['random_state']= 0\n",
    "gbm1 = lightgbm.sklearn.LGBMClassifier(**lgb_params)\n",
    "\n",
    "lgb_params_2 = {\n",
    "    'learning_rate': 0.005,\n",
    "    'n_estimators': 3700,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 2,\n",
    "    'colsample_bytree': 0.3,  \n",
    "    'num_leaves': 16\n",
    "}\n",
    "\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 4\n",
    "}\n",
    "\n",
    "lgb_params_4 = {\n",
    "    'learning_rate':0.05,\n",
    "    'n_estimators':600,\n",
    "    'num_leaves':35,\n",
    "    'min_child_samples':500\n",
    "}\n",
    "\n",
    "# lgb_params_5 = {\n",
    "#     'learning_rate':0.01,\n",
    "#     'n_estimators':1300,\n",
    "#     'num_leaves':25,\n",
    "#     'subsample': 0.8,\n",
    "#     'subsample_freq': 10,\n",
    "#     'colsample_bytree': 0.8 ,  \n",
    "#     'min_child_samples': 500,\n",
    "#     'num_leaves': 25\n",
    "# }\n",
    "\n",
    "xgb_params= {'max_depth':5, \n",
    "             'learning_rate':0.01, \n",
    "             'n_estimators':1300, \n",
    "             'silent':True, \n",
    "             'objective':'binary:logistic', \n",
    "             'nthread':3, \n",
    "             'gamma':0,\n",
    "             'min_child_weight':500, \n",
    "             'max_delta_step':0, \n",
    "             'subsample':1, \n",
    "             'colsample_bytree':0.8, \n",
    "             'colsample_bylevel':1, \n",
    "             'reg_alpha':0, \n",
    "             'reg_lambda':1, \n",
    "             'scale_pos_weight':20, \n",
    "             'base_score':0.05, \n",
    "             'seed':1234, \n",
    "             'missing':-1\n",
    "            }\n",
    "xgbm1= xgboost.sklearn.XGBClassifier(**xgb_params)\n",
    "\n",
    "\n",
    "gbm2 = lightgbm.sklearn.LGBMClassifier(**lgb_params_2)\n",
    "gbm3 = lightgbm.sklearn.LGBMClassifier(**lgb_params_3)\n",
    "gbm4 = lightgbm.sklearn.LGBMClassifier(**lgb_params_4)\n",
    "# gbm5 = lightgbm.sklearn.LGBMClassifier(**lgb_params_5)\n",
    "\n",
    "# xgbm1.fit(X_train,y_train)\n",
    "# y_pred= xgbm1.predict_proba(X_test)[:,1]\n",
    "\n",
    "import sklearn.neural_network\n",
    "mlp_params= {\n",
    "    'hidden_layer_sizes':(100,100), \n",
    "    'activation':'relu', \n",
    "    'solver':'adam', \n",
    "    'alpha':0.00000001, \n",
    "    'batch_size':'auto', \n",
    "    'learning_rate_init':0.001, \n",
    "    'shuffle':True, \n",
    "    'tol':0.0001, \n",
    "    'early_stopping': True,\n",
    "    'validation_fraction':0.1, \n",
    "    'beta_1':0.9, \n",
    "    'beta_2':0.999, \n",
    "    'epsilon':1e-08\n",
    "}\n",
    "mlp= sklearn.neural_network.MLPClassifier(**mlp_params)\n",
    "#mlp.fit(X_train,y_train)\n",
    "#y_pred= mlp.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "lr_base_params= {\n",
    "    'C':100.0,\n",
    "     'penalty':'l2',\n",
    "    'class_weight':'balanced'\n",
    "}\n",
    "# lr_base= sklearn.linear_model.LogisticRegression(**lr_base_params)\n",
    "#lr_base.fit(X_train,y_train)\n",
    "#y_pred= lr_base.predict_proba(X_test)[:,1]\n",
    "\n",
    "lr_stacker= sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "# lgbm_stacker= lightgbm.sklearn.LGBMClassifier(**{'learning_rate':0.01,\n",
    "#                                                  'n_estimators':500,\n",
    "#                                                  'num_leaves':6,\n",
    "#                                                  'min_child_samples':500})\n",
    "\n",
    "#import sklearn.ensemble\n",
    "#rf_stacker= sklearn.ensemble.RandomForestClassifier(n_estimators=300,\n",
    "#                                                   max_depth=7,\n",
    "#                                                   min_samples_leaf= 300)\n",
    "\n",
    "# knnparams={\n",
    "#     'n_neighbors':5, \n",
    "#     'weights':'uniform', \n",
    "#     'algorithm':'kd_tree'\n",
    "# }\n",
    "# import sklearn.neighbors\n",
    "# knn= sklearn.neighbors.KNeighborsClassifier(**knnparams)\n",
    "# knn.fit(X_train,y_train)\n",
    "# y_pred= knn.predict_proba(X_test)\n",
    "\n",
    "tc= datetime.now()\n",
    "stack = Ensemble(n_splits=3,\n",
    "stacker = lr_stacker,\n",
    "        base_models = (gbm1, gbm2, gbm3, gbm4))#, mlp, lr_base))        \n",
    "y_pred = stack.fit_predict(X_train, y_train, X_test) \n",
    "\n",
    "print 'Training took: ',(datetime.now() - tc).total_seconds(),' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob= y_pred #gbm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC=  0.643316693409\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "print 'AUC= ',sklearn.metrics.roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28663338681789985"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.*sklearn.metrics.roc_auc_score(y_test,y_pred)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28558503453403355"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini_normalized(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With xgboost, 2352 secs training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.121645\n",
      "6 0.106344\n",
      "10 0.0862979\n",
      "1 0.080489\n",
      "3 0.0571396\n",
      "0 0.0547097\n",
      "4 0.0477619\n",
      "5 0.046433\n",
      "12 0.0417252\n",
      "11 0.0370553\n",
      "13 0.0353848\n",
      "8 0.0332587\n",
      "7 0.0241467\n",
      "68 0.0130225\n",
      "55 0.0115418\n",
      "23 0.0109344\n",
      "69 0.00945366\n",
      "16 0.00903603\n",
      "20 0.0088462\n",
      "15 0.00880823\n",
      "63 0.0086184\n",
      "66 0.00812483\n",
      "33 0.00812483\n",
      "64 0.00774517\n",
      "57 0.00766924\n",
      "56 0.00744144\n",
      "29 0.00721364\n",
      "25 0.00694787\n",
      "21 0.00664414\n",
      "48 0.00615058\n",
      "65 0.00573294\n",
      "35 0.00561904\n",
      "52 0.00554311\n",
      "36 0.00508751\n",
      "37 0.00482175\n",
      "38 0.00482175\n",
      "70 0.00466988\n",
      "31 0.00444208\n",
      "46 0.00417632\n",
      "50 0.00391055\n",
      "47 0.00391055\n",
      "51 0.00391055\n",
      "26 0.00364479\n",
      "44 0.00330309\n",
      "58 0.00311325\n",
      "17 0.00201223\n",
      "59 0.00189833\n",
      "27 0.00170849\n",
      "34 0.00151866\n",
      "42 0.00140476\n",
      "53 0.00121493\n",
      "24 0.00121493\n",
      "61 0.00087323\n",
      "32 0.000797297\n",
      "49 0.000683397\n",
      "45 0.000607464\n",
      "18 0.000493565\n",
      "22 0.0001139\n",
      "60 3.79665e-05\n",
      "54 0.0\n",
      "2 0.0\n",
      "67 0.0\n",
      "14 0.0\n",
      "43 0.0\n",
      "62 0.0\n",
      "28 0.0\n",
      "40 0.0\n",
      "41 0.0\n",
      "39 0.0\n",
      "19 0.0\n",
      "30 0.0\n"
     ]
    }
   ],
   "source": [
    "yoyo= np.argsort(-xgbm1.feature_importances_)\n",
    "for i in yoyo:\n",
    "    print i, xgbm1.feature_importances_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(stack,open('simple_stacker.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
